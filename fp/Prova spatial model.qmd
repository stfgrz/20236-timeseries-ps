---
title: "Prova spatial model"
author: "Stefano Graziosi"
format: html
editor: visual
---

# Version 4

This R script demonstrates how to build a multistation state-space model (Dynamic Linear Model, **DLM**) to forecast daily minimum and maximum temperatures in San Francisco (SF). We use the `dlm` package (Petris et al., 2009) to construct a Bayesian state-space model that **focuses on SF** but **borrows strength from nearby stations**. The model incorporates seasonal components (to capture annual cycles) and handles missing observations via the Kalman filter. Parameter estimation is done in a Bayesian framework (using maximum likelihood or Gibbs sampling for variance components as outlined by Petris et al.). Below, we provide well-documented R code for data preparation, model construction, filtering, smoothing, forecasting (3-day, 7-day, 14-day horizons), and plotting of predictions.

### 3.3.1 Data Preparation and Station Selection

First, we load the daily temperature data and prepare it for modeling. We assume `ghcn.csv` contains daily observations for SF and other stations (e.g., nearby California locations). We filter the data to include SF and a handful of neighboring stations so that the model can leverage their data. If the data is in long format (rows for each station-date-variable), we pivot it to a **wide format** with each column as a station’s time series. Missing values (NA) are allowed and will be handled by the Kalman filter.

```{r}
# Load required package
library(dlm)        # for Dynamic Linear Models
library(tidyverse)  # for data manipulation (dplyr, tidyr, ggplot2, etc.)
library(lubridate)  # for working with Dates

# 1b) Read (or re‐read) the GHCN CSV (you already had this, but for clarity):
ghcn <- read.csv("fp_data/ghcn.txt", header = TRUE)
colnames(ghcn) <- c(
  "station_id",
  "station_name",
  "latitude",
  "longitude",
  "elevation_m",
  "date",
  "tmin_tenthC",
  "tmax_tenthC",
  "tavg_tenthC",
  "prcp_tenthmm"
)

# 1c) Convert types, compute actual Celsius / mm, and drop the “_tenth” fields:
ghcn <- ghcn %>%
  mutate(
    date  = as_date(date),                   # convert YYYY-MM-DD → Date
    tmin  = tmin_tenthC / 10,                # tenths of °C → °C
    tmax  = tmax_tenthC / 10,                # tenths of °C → °C
    prcp  = prcp_tenthmm / 10                 # tenths of mm → mm
  ) %>%
  select(
    station_id, station_name, latitude, longitude, elevation_m,
    date, tmin, tmax, prcp
  )

ghcn <- subset(ghcn, date >= as.Date("1976-11-01"))

# Filter data for those stations and pivot to wide format (Date as index):
library(tidyr)
library(dplyr)

# Pivot to wide: create separate wide data for TMIN and TMAX
tmin_wide <- ghcn %>%
  select(date, station_name, tmin) %>%
  pivot_wider(names_from = station_name, values_from = tmin)

tmax_wide <- ghcn %>%
  select(date, station_name, tmax) %>%
  pivot_wider(names_from = station_name, values_from = tmax)

# Ensure dates are sorted and aligned
tmin_wide <- arrange(tmin_wide, date)
tmax_wide <- arrange(tmax_wide, date)

# Extract the matrix of values (exclude Date column)
Y_min <- as.matrix(tmin_wide[,-1])   # rows: dates, cols: stations
Y_max <- as.matrix(tmax_wide[,-1])

# Set row names as date for clarity (optional)
rownames(Y_min) <- tmin_wide$date
rownames(Y_max) <- tmax_wide$date

# Confirm dimensions and presence of NAs
dim(Y_min); dim(Y_max)
colnames(Y_min)  # should list station names in target_stations order
summary(Y_min)   # check for missing values

# For modeling, we'll proceed with Y_max (daily max temp) for demonstration.
Y <- Y_max   # use daily maximum temperature; similarly, Y_min can be modeled
```

**Notes:** We chose a set of nearby stations (including SF) to help forecast SF’s temperatures. Having multiple stations’ data in `Y` allows the state-space model to exploit their shared trends and seasonal patterns. The data is now in a matrix `Y` of size `[time_points × number_of_stations]`. Missing values (`NA`) in `Y` are acceptable – the Kalman filter will automatically handle them by skipping updates for those entries.

### 3.3.2 Model Specification: State-Space Structure

We construct a **Dynamic Linear Model** for the multistation temperature data. The state vector is designed to capture shared dynamics across stations as well as station-specific effects. Key state components include:

-   **Global Level (Trend):** A local level representing the underlying regional temperature signal (centered on SF). This captures day-to-day fluctuations (e.g., weather anomalies) common to all stations. We allow it to evolve via a random walk (with variance *W_level* to be estimated).
-   **Seasonal Harmonics (Annual Cycle):** We include a seasonal component to model the annual temperature cycle (period \~365 days). We use a Fourier form seasonal model with a few harmonics (e.g., 1st and 2nd harmonics) for parsimony. This captures the yearly temperature pattern (summer highs, winter lows) while allowing slow variation year-to-year by assigning a small evolution variance.
-   **Station Biases:** Each station (except SF as baseline) gets a constant bias term to account for its average difference from SF (due to elevation, coastal vs inland, etc.). These biases are included as state components (one per station) and can be treated as either fixed offsets (zero evolution variance) or as very slowly varying (small *W_bias*). This ensures the model’s shared level + seasonal pattern is adjusted to each station’s mean.

The observation equation links these states to the actual station temperatures. **San Francisco’s temperature** is modeled as: $\text{SF}_t = L_t + S_t + \varepsilon_{SF,t},$ where $L_t$ is the global level and $S_t$ is the seasonal effect (sum of harmonics) at time *t*. For another station *j*, the observation is: $Y_{j,t} = L_t + S_t + b_{j,t} + \varepsilon_{j,t},$ where $b_{j,t}$ is the bias state for station *j*. If bias states are static (no evolution noise), then $b_{j,t} = b_j$ (constant offset). All $\varepsilon_{*,t}$ are observation errors (assumed Gaussian white noise) – we will allow for a station-specific or common observation variance *V*. The state evolution for $L_t$ (and an optional slope) is a local trend model, for seasonal states is a rotation each day (with small variance), and for biases is identity (persistence).

Below we construct the `dlm` model matrices for this structure:

```{r}
# Determine model dimensions
n_stations <- ncol(Y)                     # number of stations (m)
station_names <- colnames(Y)
# Assume station_names[1] == "SAN FRANCISCO" (the target), used as baseline
sf_idx <- which(station_names == "SAN FRANCISCO DWTN")
new_order <- c(sf_idx, setdiff(seq_along(station_names), sf_idx))
Y <- Y[, new_order]
station_names <- station_names[new_order]


# 2.1 Build a single‐output DLM for (trend + seasonal + AR(1)):
#     - Keep the same local linear trend
#     - Keep the same Fourier seasonal
#     - Add an AR(1) piece with parameter phi (to be estimated)
#    We freeze the slope‐variance and seasonal variance to small numbers still,
#    but now we let the AR(1) variance be estimated so that the filter can capture
#    short‐term autocorrelation.

base_trend  <- dlmModPoly(order = 2, dV = 0, dW = c(1e-7, 1e-7))
base_season <- dlmModTrig(s = 365, q = 2, dV = 0, dW = 1e-7)
base_ar     <- dlmModARMA(ar = 0.9, ma = NULL, dV = 0, dW = 0.1)
#   --> This “ARMA” block is a pure AR(1) with phi=0.9 initial guess; dW=0.1 initial.
#   The internal state‐dimension of dlmModARMA(ar=1) is 1.

single_model_with_AR <- base_trend + base_season + base_ar
m_single_AR <- ncol(single_model_with_AR$FF)  # e.g. 2 (trend) + 4 (season) + 1 (AR) = 7 states

# 2.2 Now expand to the multistation version EXACTLY as before:
#     - We put m_single_AR “common” states into the top‐left corner of a big DLM
#     - We append one “bias state” for stations 2..m
m_base_AR   <- m_single_AR
n_bias      <- n_stations - 1
total_states_AR <- m_base_AR + n_bias

# Observation matrix:  
FF_AR <- matrix(0, nrow = n_stations, ncol = total_states_AR)
for(i in 1:n_stations) {
  FF_AR[i, 1:m_base_AR] <- single_model_with_AR$FF
}
# Add station‐bias columns just like before:
if(n_bias > 0) {
  for(j in 2:n_stations) {
    bias_col <- m_base_AR + (j - 1)
    FF_AR[j, bias_col] <- 1
  }
}

# State‐transition:
GG_AR <- diag(1, total_states_AR)
GG_AR[1:m_base_AR, 1:m_base_AR] <- single_model_with_AR$GG
# Bias states remain “identity”:
#   (i.e. b_{j,t+1} = b_{j,t} + bias_noise)

# System noise W:
W_AR <- matrix(0, nrow = total_states_AR, ncol = total_states_AR)
W_AR[1:m_base_AR, 1:m_base_AR] <- single_model_with_AR$W
# Freeze bias noise small:
if(n_bias > 0) {
  diag(W_AR[(m_base_AR+1):total_states_AR, (m_base_AR+1):total_states_AR]) <- 1e-7
}

# Observation noise V: start with identity, to be estimated:
V_AR <- diag(1, n_stations)

# Initial state prior:
m0_AR <- rep(0, total_states_AR)
# If you want to initialize bias around first‐day difference again:
if(n_bias > 0) {
  sf0 <- Y[1,1]
  for(j in 2:n_stations) {
    if(!is.na(Y[1,j]) && !is.na(sf0)) {
      m0_AR[m_base_AR + (j-1)] <- Y[1,j] - sf0
    }
  }
}
C0_AR <- diag(1e6, total_states_AR)

# Build the “base” with all W and V frozen at initial guesses:
mod_multi_AR <- dlm(m0 = m0_AR, C0 = C0_AR, FF = FF_AR, V = V_AR, GG = GG_AR, W = W_AR)

# 2.3 Now define an MLE‐builder that estimates:
#     - phi (AR1 coefficient, inside the ARMA block)
#     - W_ar  (the AR‐block’s variance)
#     - W_level (the local‐trend’s level variance)
#     - V_obs   (common obs variance)
buildModel_withAR <- function(param) {
  # param = c(logit(phi_unc), log(W_ar), log(W_level), log(V_obs))
  # Actually we’ll do: phi = 2*exp(param[1])/(1+exp(param[1])) - 1
  # so phi is constrained to (-1, +1).  That’s a “logit‐like” transform for AR1.
  phi_raw   <- exp(param[1])
  phi       <- (2 * phi_raw / (1 + phi_raw)) - 1   # phi in (-1,1)
  W_ar      <- exp(param[2])                      # AR(1) process noise
  W_level   <- exp(param[3])                      # trend noise
  V_obs     <- exp(param[4])                      # observation noise

  mod_tmp <- mod_multi_AR
  # Insert phi into the AR block in the big GG:
  #   Recall that in single_model_with_AR, the AR(1) block is a 1×1 at index:
  #   it is the *last* row/col of the single‐output GG.  That corresponds to
  #   state‐number = m_base_AR (the AR is the m_base_AR-th state in single_model_with_AR).
  #   But in our multistation GG_AR, the AR block occupies rows/cols 3.. 
  #   Actually, if base_trend was 2 dims, seasonal 4 dims, AR is 1 dim, so
  #   the AR index in the combined “common” block is:
  ar_index <- m_base_AR  # because base_trend (2) + season (4) = 6, so AR is index=7
  # In the multistation model, that AR block sits at [ar_index, ar_index].
  GG_AR_subset <- single_model_with_AR$GG
  # replace the AR‐entry
  GG_AR_subset[ar_index, ar_index] <- phi
  mod_tmp$GG[1:m_base_AR, 1:m_base_AR] <- GG_AR_subset

  # Insert AR variances:
  #   single_model_with_AR$W has the correct structure for trend+season+AR noise
  mod_tmp$W[1:m_base_AR, 1:m_base_AR] <- single_model_with_AR$W
  #   But we need to replace the AR’s own variance (in single_model_with_AR$W)
  mod_tmp$W[ar_index, ar_index] <- W_ar

  # Insert level’s variance:
  mod_tmp$W[1,1] <- W_level    # keep slope variance frozen at 1e-7

  # Leave seasonal states’ W at 1e-7 (as it was), and biases at 1e-7
  # Now set observation noise on the diagonal:
  diag(mod_tmp$V) <- V_obs

  mod_tmp
}

# 2.4 Choose initial guesses for (phi, W_ar, W_level, V_obs):
#     - phi around 0.8 (temperature has strong one‐day lag)
#     - W_ar small (like 0.1)
#     - W_level small (0.01)
#     - V_obs small (0.5)
init_par_AR <- c(
  log( (1 + 0.8)/(1 - 0.8) ),  # fisher‐like initial guess for phi=0.8 -> logit((phi+1)/2)
  log(0.1),                   # W_ar
  log(0.01),                  # W_level
  log(0.5)                    # V_obs
)

# Run MLE:
mle_AR <- dlmMLE(Y, parm = init_par_AR, build = buildModel_withAR, control = list(maxit=2000))
stopifnot(mle_AR$convergence == 0)
opt_par_AR <- mle_AR$par
# Transform back to interpretable scale:
phi_hat    <- (2 * exp(opt_par_AR[1]) / (1 + exp(opt_par_AR[1]))) - 1
W_ar_hat   <- exp(opt_par_AR[2])
W_level_hat<- exp(opt_par_AR[3])
V_obs_hat  <- exp(opt_par_AR[4])
cat("φ̂ = ", phi_hat, "\n",
    "W_ar = ", W_ar_hat, "\n",
    "W_level = ", W_level_hat, "\n",
    "V_obs = ", V_obs_hat, "\n")

# Rebuild the final fitted model:
mod_fitted_AR <- buildModel_withAR(opt_par_AR)

```

**Explanation:** We built the multivariate DLM in steps. The **base model** `base_model` represents a single-output DLM for the shared SF temperature signal: it has a local linear trend (order 2) and an annual seasonal component with two harmonics. The Fourier seasonal component provides a parsimonious representation of a yearly cycle, and we allow it to evolve slowly by giving it a tiny process variance. Next, we expanded this model to multiple stations by adding bias states. The `FF_multi` matrix is defined such that each station’s observation is the sum of the common trend+season and its own bias (zero for SF). The `GG_multi` matrix keeps the common states evolving as before and makes each bias state follow a **random walk** (identity transition). We initialized the process noise `W_multi` with small variances for all states (to be tuned) and the observation noise `V_multi` as identity (to be estimated). A diffuse prior (large `C0`) is used so that the data can inform the state estimates. **Missing data** is handled implicitly: if some `Y[t,j]` is NA, the Kalman filter will simply not update that station’s component at time *t*, effectively carrying forward the predicted state for that observation.

### 3.3.3 Bayesian Parameter Inference (Variance Estimation)

The DLM’s structure is specified, but certain hyperparameters (variance terms in `W` and `V`) are unknown and need to be learned. Following Petris et al. (2009), we can estimate these using **Bayesian inference**. One approach is to put prior distributions on the variances and run a Gibbs sampler (`dlmGibbsDIG`) to sample from their posterior. For simplicity, here we perform **Maximum Likelihood Estimation (MLE)** of the variances, which corresponds to finding posterior mode estimates (an empirical Bayes approach). We use `dlmMLE` to optimize the log-likelihood of the model with respect to selected variance parameters, then update the DLM with these estimates.

In the code below, we choose to estimate the global level's evolution variance and the observation noise variance, while keeping other variances fixed at small values. (We could also estimate more parameters – e.g. slope variance, seasonal variance, or separate noise per station – but to avoid overfitting, we start simple.) The parameters are optimized in log-space to enforce positivity.

```{r}
# Define a function that builds the DLM given a parameter vector (to be optimized)
# For example, let param[1] = log(W_level), param[2] = log(V_obs) as the parameters to estimate.
buildModel <- function(param) {
  # Extract parameters (ensure they remain positive by exponentiating)
  W_level <- exp(param[1])
  V_obs_raw   <- exp(param[2])
  # Force a maximum V_obs of 0.5:  
  V_obs <- min(V_obs_raw, 0.5)  
  
  # Update copy of mod_multi with these parameters
  mod_temp <- mod_multi  # start from current model structure
  # Set level process variance (state 1 corresponds to level in our state ordering)
  mod_temp$W[1, 1] <- W_level
  # Optionally, if we included a slope (state 2), we might set mod_temp$W[2, 2] to a small value or estimate it too.
  # (Here we keep slope variance as a small fixed value from earlier, e.g., 1e-7)
  
  # Set all observation variances (or just SF's) to V_obs
  # Here we assume a common observation variance for simplicity
  diag(mod_temp$V) <- V_obs
  
  return(mod_temp)
}

# Initial guesses for log-variances
init_param <- log(c(W_level = 0.1, V_obs = 1))  # e.g., initial guess: W_level=0.1, V=1

# Run MLE optimization
mle_fit <- dlmMLE(Y, parm = init_param, build = buildModel)
mle_fit$convergence  # 0 indicates successful convergence

# Optimized parameter estimates (back-transformed)
opt_param <- exp(mle_fit$par)
opt_param_names <- c("W_level", "V_obs")
names(opt_param) <- opt_param_names
print(opt_param)
# e.g., might output something like W_level ~ 0.05, V_obs ~ 2.3 (just as an example)

# Rebuild the DLM with estimated parameters
mod_fitted <- buildModel(mle_fit$par)
```

After MLE, `opt_param` contains the estimated variances. We then reconstruct `mod_fitted`, the DLM with these optimal values. At this stage, our model is fully specified. We have essentially **learned the variance hyperparameters** from data (a point estimate). If a fully Bayesian treatment is desired, we could set inverse-Gamma priors for these variances and use `dlmGibbsDIG` to sample their posterior distribution, but for brevity we proceed with the MLE estimates. The state estimation and forecasting that follow still use Bayesian updating (the Kalman filter), treating the variances as known.

### 3.3.4 Filtering and Smoothing (State Estimation)

With the fitted model, we perform Kalman filtering to estimate the latent states over time, and Kalman smoothing to refine those estimates using all data. Filtering yields one-step ahead forecasts and updated state estimates at each time, while smoothing provides retrospective estimates of states using future data as well.

```{r}
# Apply Kalman filter to the multivariate series
filter_res <- dlmFilter(Y, mod_fitted)

# Extract filtered one-step ahead forecasts and residuals (optional)
y_hat <- filter_res$f    # one-step ahead predicted observations (matrix: same dim as Y)
y_err <- filter_res$y - y_hat  # residuals (differences) wherever data is not NA

# Extract filtered state estimates
filtered_states_mean <- filter_res$m    # matrix of filtered state means (each row = time, col = state)
filtered_states_var  <- filter_res$U    # variance (upper triangular form; use dlmSvd2var to get cov matrices)
# Note: dlmFilter stores state variance in SVD form (U and D components). To get actual covariance at time n:
final_state_cov <- dlmSvd2var(filter_res$U.R, filter_res$D.R)  # covariance of final state

# Apply Rauch-Tung-Striebel smoother for retrospective state estimates
smooth_res <- dlmSmooth(filter_res)

smoothed_states_mean <- smooth_res$s    # matrix of smoothed state means (each row = time, col = state)
smoothed_states_var  <- smooth_res$U    # (in SVD form similarly)
```

After filtering (`dlmFilter`), the object `filter_res` contains the filtered estimates. In particular, `filter_res$m` is a matrix where each row gives the mean of the state vector at the end of that day *t* (after observing *Y\[t,*\]\*). The Kalman filter naturally handled missing observations: if SF or any station had NA on a given day, the filter simply relied on the state prediction (no update for that component). The `dlmSmooth` results combine forward and backward passes to provide smoothed states `smooth_res$s`, which are more accurate estimates of the true latent states using all available data.

**Interpretation Tip:** The first component of the state (Level) represents the **common temperature signal** (centered on SF). The seasonal components (next 4 states) represent the estimated seasonal cycle (Fourier coefficients). The remaining state components are the **station biases**. By examining `smoothed_states_mean`, one can see, for example, the bias states converging to values that reflect each station’s average offset relative to SF.

### 3.3.5 Forecasting 3-Day, 7-Day, and 14-Day Ahead

Finally, we use the fitted and filtered model to predict future temperatures. We will produce **short-term forecasts** for SF’s minimum and maximum temperature for the next 3, 7, and 14 days. The `dlmForecast` function provides the forecasted means and variances for any number of steps ahead. We focus on SF (first station), but the function actually forecasts all stations simultaneously (leveraging the state structure).

```{r}
# Forecast 14 days ahead using the filtered state as the starting point
n_ahead <- 14
forecast_res <- dlmForecast(filter_res, nAhead = n_ahead)

# Extract forecast means for all stations
Y_fore_mean <- forecast_res$f   # a list or matrix of forecast means (nAhead x n_stations)
# Extract forecast covariance matrices for each forecast step
Y_fore_cov <- forecast_res$Q    # list of length nAhead, each an (n_stations x n_stations) covariance matrix

# Focus on San Francisco (column 1)
sf_fore_mean <- if(is.list(Y_fore_mean)) {
  # dlmForecast prior to dlm v1.1 returns a list; newer versions return matrix
  do.call(rbind, Y_fore_mean)[,1]
} else {
  Y_fore_mean[,1]
}

# Compute prediction intervals for SF forecasts (e.g., 90% interval)
alpha <- 0.10  # for 90% interval (10% in lower tail, 10% in upper tail)
# Get standard deviation of SF forecast at each horizon
sf_fore_sd <- sapply(Y_fore_cov, function(Sig) sqrt(Sig[1,1]))
z <- qnorm(1 - alpha/2)  # z for central (1-alpha)% interval (e.g., 1.6449 for 90%)
sf_fore_lower <- sf_fore_mean - z * sf_fore_sd
sf_fore_upper <- sf_fore_mean + z * sf_fore_sd

# Print the 3-day, 7-day, 14-day ahead forecast for SF Max Temperature:
horizons <- c(3, 7, 14)
forecast_table <- data.frame(
  Horizon = horizons,
  Forecast_Date = as.Date(tail(tmax_wide$date, 1)) + horizons,  # last date + horizon
  SF_Max_Temp_Forecast = sf_fore_mean[horizons],
  Lower90 = sf_fore_lower[horizons],
  Upper90 = sf_fore_upper[horizons]
)
print(forecast_table)
```

This yields a table of SF’s forecasted max temperatures for the next 3, 7, and 14 days, along with 90% prediction intervals. (For minimum temperatures, we would repeat the modeling with `Y_min` or build a similar model for TMIN. Often TMIN and TMAX are modeled separately due to differing daily dynamics, but the procedure is analogous.)

**Forecast interpretation:** The model’s forecasts combine the last estimated state (which encodes the recent level of temperature and seasonality) with its dynamics. For example, if the last observed day was unusually warm, the **global level state** $L_t$ will be higher, carrying that signal into the forecast for subsequent days (gradually decaying if the model expects mean reversion via the level’s variance). The seasonal states ensure the forecast respects the time of year (e.g., rising trend into summer, or falling into winter). The **borrowed strength** from other stations means if SF had missing data or anomalous behavior, the model still anchors to the regional pattern gleaned from neighbors.

### 3.3.6 Plotting the Predictions

Finally, we plot the forecast results for visualization. We show the observed SF temperature series (recent period) and the 14-day forecast with its prediction interval.

```{r}
# Plot the last 60 days of observed SF data and the 14-day forecast
sf_actual <- Y[, 1]  # SF historical series (max temp)
date_index <- as.Date(tmax_wide$date)   # actual dates in the data
end_date <- tail(date_index, 1)
plot_window <- 60  # how many past days to show
plot_dates <- tail(date_index, plot_window)
plot_values <- tail(sf_actual, plot_window)

# Base plot for historical data
plot(plot_dates, plot_values, type="l", col="black", lwd=2,
     xlim = c(min(plot_dates), end_date + n_ahead),
     ylim = range(c(plot_values, sf_fore_lower, sf_fore_upper), na.rm=TRUE),
     xlab="Date", ylab="Temperature (°C)", 
     main="San Francisco Daily Max Temperature: Forecast for 14 Days Ahead")
grid()

# Add forecast mean line and confidence interval
fore_dates <- seq(end_date + 1, by = "day", length.out = n_ahead)
lines(fore_dates, sf_fore_mean, col="steelblue", lwd=2, lty=1)
# Shaded prediction interval (90%)
polygon(x = c(fore_dates, rev(fore_dates)),
        y = c(sf_fore_lower, rev(sf_fore_upper)),
        col = rgb(0, 0, 1, 0.2), border = NA)
lines(fore_dates, sf_fore_lower, col="steelblue", lty=2, lwd=1)
lines(fore_dates, sf_fore_upper, col="steelblue", lty=2, lwd=1)

# Add legend
legend("topleft", 
       legend = c("Observed SF Max Temp", "Forecast Mean", "90% Prediction Interval"), 
       col = c("black", "steelblue", "steelblue"), lty = c(1, 1, 2), lwd = c(2, 2, 1), bg = "white")
```

# Version 3

Great. I’ll develop R code for a multistation state space model using the `dlm` package that focuses on forecasting San Francisco's daily minimum and maximum temperatures. The model will incorporate seasonal components and allow borrowing strength from other stations. I’ll make forecasts for 3-day, 7-day, and 14-day horizons, and handle missing data in a robust way consistent with the methodology from the *Dynamic Linear Models with R* book.

I’ll let you know when the model specification and code are ready.

# Multistation Dynamic Linear Model for San Francisco Temperature Forecasting

This R script demonstrates how to build a multistation state-space model (Dynamic Linear Model, **DLM**) to forecast daily minimum and maximum temperatures in San Francisco (SF). We use the `dlm` package (Petris et al., 2009) to construct a Bayesian state-space model that **focuses on SF** but **borrows strength from nearby stations**. The model incorporates seasonal components (to capture annual cycles) and handles missing observations via the Kalman filter. Parameter estimation is done in a Bayesian framework (using maximum likelihood or Gibbs sampling for variance components as outlined by Petris et al.). Below, we provide well-documented R code for data preparation, model construction, filtering, smoothing, forecasting (3-day, 7-day, 14-day horizons), and plotting of predictions.

### 3.3.1 Data Preparation and Station Selection

First, we load the daily temperature data and prepare it for modeling. We assume `ghcn.csv` contains daily observations for SF and other stations (e.g., nearby California locations). We filter the data to include SF and a handful of neighboring stations so that the model can leverage their data. If the data is in long format (rows for each station-date-variable), we pivot it to a **wide format** with each column as a station’s time series. Missing values (NA) are allowed and will be handled by the Kalman filter.

```{r}
# Load required package
library(dlm)        # for Dynamic Linear Models
library(tidyverse)  # for data manipulation (dplyr, tidyr, ggplot2, etc.)
library(lubridate)  # for working with Dates

# 1b) Read (or re‐read) the GHCN CSV (you already had this, but for clarity):
ghcn <- read.csv("fp_data/ghcn.txt", header = TRUE)
colnames(ghcn) <- c(
  "station_id",
  "station_name",
  "latitude",
  "longitude",
  "elevation_m",
  "date",
  "tmin_tenthC",
  "tmax_tenthC",
  "tavg_tenthC",
  "prcp_tenthmm"
)

# 1c) Convert types, compute actual Celsius / mm, and drop the “_tenth” fields:
ghcn <- ghcn %>%
  mutate(
    date  = as_date(date),                   # convert YYYY-MM-DD → Date
    tmin  = tmin_tenthC / 10,                # tenths of °C → °C
    tmax  = tmax_tenthC / 10,                # tenths of °C → °C
    prcp  = prcp_tenthmm / 10                 # tenths of mm → mm
  ) %>%
  select(
    station_id, station_name, latitude, longitude, elevation_m,
    date, tmin, tmax, prcp
  )

ghcn <- subset(ghcn, date >= as.Date("1976-11-01"))

# Filter data for those stations and pivot to wide format (Date as index):
library(tidyr)
library(dplyr)

# Pivot to wide: create separate wide data for TMIN and TMAX
tmin_wide <- ghcn %>%
  select(date, station_name, tmin) %>%
  pivot_wider(names_from = station_name, values_from = tmin)

tmax_wide <- ghcn %>%
  select(date, station_name, tmax) %>%
  pivot_wider(names_from = station_name, values_from = tmax)

# Ensure dates are sorted and aligned
tmin_wide <- arrange(tmin_wide, date)
tmax_wide <- arrange(tmax_wide, date)

# Extract the matrix of values (exclude Date column)
Y_min <- as.matrix(tmin_wide[,-1])   # rows: dates, cols: stations
Y_max <- as.matrix(tmax_wide[,-1])

# Set row names as date for clarity (optional)
rownames(Y_min) <- tmin_wide$date
rownames(Y_max) <- tmax_wide$date

# Confirm dimensions and presence of NAs
dim(Y_min); dim(Y_max)
colnames(Y_min)  # should list station names in target_stations order
summary(Y_min)   # check for missing values

# For modeling, we'll proceed with Y_max (daily max temp) for demonstration.
Y <- Y_max   # use daily maximum temperature; similarly, Y_min can be modeled
```

**Notes:** We chose a set of nearby stations (including SF) to help forecast SF’s temperatures. Having multiple stations’ data in `Y` allows the state-space model to exploit their shared trends and seasonal patterns. The data is now in a matrix `Y` of size `[time_points × number_of_stations]`. Missing values (`NA`) in `Y` are acceptable – the Kalman filter will automatically handle them by skipping updates for those entries.

### 3.3.2 Model Specification: State-Space Structure

We construct a **Dynamic Linear Model** for the multistation temperature data. The state vector is designed to capture shared dynamics across stations as well as station-specific effects. Key state components include:

-   **Global Level (Trend):** A local level representing the underlying regional temperature signal (centered on SF). This captures day-to-day fluctuations (e.g., weather anomalies) common to all stations. We allow it to evolve via a random walk (with variance *W_level* to be estimated).
-   **Seasonal Harmonics (Annual Cycle):** We include a seasonal component to model the annual temperature cycle (period \~365 days). We use a Fourier form seasonal model with a few harmonics (e.g., 1st and 2nd harmonics) for parsimony. This captures the yearly temperature pattern (summer highs, winter lows) while allowing slow variation year-to-year by assigning a small evolution variance.
-   **Station Biases:** Each station (except SF as baseline) gets a constant bias term to account for its average difference from SF (due to elevation, coastal vs inland, etc.). These biases are included as state components (one per station) and can be treated as either fixed offsets (zero evolution variance) or as very slowly varying (small *W_bias*). This ensures the model’s shared level + seasonal pattern is adjusted to each station’s mean.

The observation equation links these states to the actual station temperatures. **San Francisco’s temperature** is modeled as: $\text{SF}_t = L_t + S_t + \varepsilon_{SF,t},$ where $L_t$ is the global level and $S_t$ is the seasonal effect (sum of harmonics) at time *t*. For another station *j*, the observation is: $Y_{j,t} = L_t + S_t + b_{j,t} + \varepsilon_{j,t},$ where $b_{j,t}$ is the bias state for station *j*. If bias states are static (no evolution noise), then $b_{j,t} = b_j$ (constant offset). All $\varepsilon_{*,t}$ are observation errors (assumed Gaussian white noise) – we will allow for a station-specific or common observation variance *V*. The state evolution for $L_t$ (and an optional slope) is a local trend model, for seasonal states is a rotation each day (with small variance), and for biases is identity (persistence).

Below we construct the `dlm` model matrices for this structure:

```{r}
# Determine model dimensions
n_stations <- ncol(Y)                     # number of stations (m)
station_names <- colnames(Y)
# Assume station_names[1] == "SAN FRANCISCO" (the target), used as baseline
sf_idx <- which(station_names == "SAN FRANCISCO DWTN")
new_order <- c(sf_idx, setdiff(seq_along(station_names), sf_idx))
Y <- Y[, new_order]
station_names <- station_names[new_order]


# 1. Define the base model for common trend + seasonal component (single-output DLM)
#    We'll use a local linear trend (order=2: level + slope) and a Fourier seasonal component with annual period.
#    Set dV=0 for these components since observation noise will be added separately for each station.
base_trend <- dlmModPoly(order = 2, dV = 0, dW = c(1e-7, 1e-7))  
# (Initial guess: almost no noise for level/slope; will be adjusted later. 
#  We'll estimate optimal variances via dlmMLE.)
base_season <- dlmModTrig(s = 365, q = 2, dV = 0, dW = 1e-7)  
# (Using first 2 harmonics of period 365:contentReference[oaicite:7]{index=7}. Small dW gives a slowly time-varying seasonal pattern.)
base_model <- base_trend + base_season   # Combine trend and seasonal into one single-output DLM

# Inspect state vector size of base_model
m_base <- ncol(base_model$FF)   # number of base states (level, slope, and Fourier states)
m_base  # for order=2 and q=2, m_base = 2 (trend) + 2*2 (harmonics*2) = 6 states

# 2. Augment the state vector with station-specific bias states for stations 2..m
n_bias <- n_stations - 1
total_states <- m_base + n_bias

# Initialize observation matrix FF for multi-station
FF_multi <- matrix(0, nrow = n_stations, ncol = total_states)
# Fill common part: each station observes the common trend+season sum
FF_common <- base_model$FF            # 1 x m_base (for single output) 
for (i in 1:n_stations) {
  FF_multi[i, 1:m_base] <- FF_common   # all stations see L_t + S_t
}
# Add identity for bias terms: station j (index>1) has a bias state at position m_base + (j-1)
if (n_bias > 0) {
  for (j in 2:n_stations) {
    bias_index <- m_base + (j-1)
    FF_multi[j, bias_index] <- 1      # station j gets its bias state
  }
}
# For SF (station 1), we intentionally do not add a bias term (or equivalently bias state = 0)

# Define state transition matrix GG for combined state:
# - Base states (trend + seasonal) follow base_model$GG (m_base x m_base matrix)
# - Bias states follow identity (they persist from t to t+1, possibly with small random walk)
GG_multi <- diag(1, total_states)               # start with identity
GG_multi[1:m_base, 1:m_base] <- base_model$GG   # insert base transition for common states
# (Bias states remain 1 on diagonal, 0 off-diagonals, meaning b_{j,t+1} = b_{j,t} + noise)

# System noise covariance matrix W:
W_multi <- matrix(0, nrow = total_states, ncol = total_states)
# Insert base model system covariance (trend & seasonal part)
W_multi[1:m_base, 1:m_base] <- base_model$W
# Bias states: set small variance for each bias (or 0 if treating biases as fixed)
if (n_bias > 0) {
  bias_var <- 1e-7  # initial guess: almost no drift in bias
  diag(W_multi[(m_base+1):total_states, (m_base+1):total_states]) <- bias_var
}

# Observation covariance matrix V:
# Assume independent observation errors per station. We can start with equal variances for all,
# and later allow estimation of these variances.
V_multi <- diag(1, n_stations)   # initial guess: 1 (will be adjusted by MLE)

# Initial state prior (m0 and C0):
m0 <- rep(0, total_states)
# We can set initial biases based on first observations if available (difference from SF)
if (n_bias > 0) {
  sf_initial <- Y[1, 1]
  for (j in 2:n_stations) {
    if (!is.na(Y[1,j]) && !is.na(sf_initial)) {
      m0[m_base + (j-1)] <- Y[1, j] - sf_initial  # initial bias as difference on first day
    } else {
      m0[m_base + (j-1)] <- 0  # if missing, start at 0 bias
    }
  }
}
# Set a large initial covariance for uncertainty in states
C0 <- diag(1e6, total_states)
# (High uncertainty allows the filter to learn the true state values from data. 
# If prior knowledge exists, we could reduce these variances.)

# Assemble the DLM with all components:
mod_multi <- dlm(m0 = m0, C0 = C0, FF = FF_multi, V = V_multi, GG = GG_multi, W = W_multi)
mod_multi  # print a summary of the model structure
```

**Explanation:** We built the multivariate DLM in steps. The **base model** `base_model` represents a single-output DLM for the shared SF temperature signal: it has a local linear trend (order 2) and an annual seasonal component with two harmonics. The Fourier seasonal component provides a parsimonious representation of a yearly cycle, and we allow it to evolve slowly by giving it a tiny process variance. Next, we expanded this model to multiple stations by adding bias states. The `FF_multi` matrix is defined such that each station’s observation is the sum of the common trend+season and its own bias (zero for SF). The `GG_multi` matrix keeps the common states evolving as before and makes each bias state follow a **random walk** (identity transition). We initialized the process noise `W_multi` with small variances for all states (to be tuned) and the observation noise `V_multi` as identity (to be estimated). A diffuse prior (large `C0`) is used so that the data can inform the state estimates. **Missing data** is handled implicitly: if some `Y[t,j]` is NA, the Kalman filter will simply not update that station’s component at time *t*, effectively carrying forward the predicted state for that observation.

### 3.3.3 Bayesian Parameter Inference (Variance Estimation)

The DLM’s structure is specified, but certain hyperparameters (variance terms in `W` and `V`) are unknown and need to be learned. Following Petris et al. (2009), we can estimate these using **Bayesian inference**. One approach is to put prior distributions on the variances and run a Gibbs sampler (`dlmGibbsDIG`) to sample from their posterior. For simplicity, here we perform **Maximum Likelihood Estimation (MLE)** of the variances, which corresponds to finding posterior mode estimates (an empirical Bayes approach). We use `dlmMLE` to optimize the log-likelihood of the model with respect to selected variance parameters, then update the DLM with these estimates.

In the code below, we choose to estimate the global level's evolution variance and the observation noise variance, while keeping other variances fixed at small values. (We could also estimate more parameters – e.g. slope variance, seasonal variance, or separate noise per station – but to avoid overfitting, we start simple.) The parameters are optimized in log-space to enforce positivity.

```{r}
# Define a function that builds the DLM given a parameter vector (to be optimized)
# For example, let param[1] = log(W_level), param[2] = log(V_obs) as the parameters to estimate.
buildModel <- function(param) {
  # Extract parameters (ensure they remain positive by exponentiating)
  W_level <- exp(param[1])
  V_obs_raw   <- exp(param[2])
  # Force a maximum V_obs of 0.5:  
  V_obs <- min(V_obs_raw, 0.5)  
  
  # Update copy of mod_multi with these parameters
  mod_temp <- mod_multi  # start from current model structure
  # Set level process variance (state 1 corresponds to level in our state ordering)
  mod_temp$W[1, 1] <- W_level
  # Optionally, if we included a slope (state 2), we might set mod_temp$W[2, 2] to a small value or estimate it too.
  # (Here we keep slope variance as a small fixed value from earlier, e.g., 1e-7)
  
  # Set all observation variances (or just SF's) to V_obs
  # Here we assume a common observation variance for simplicity
  diag(mod_temp$V) <- V_obs
  
  return(mod_temp)
}

# Initial guesses for log-variances
init_param <- log(c(W_level = 0.1, V_obs = 1))  # e.g., initial guess: W_level=0.1, V=1

# Run MLE optimization
mle_fit <- dlmMLE(Y, parm = init_param, build = buildModel)
mle_fit$convergence  # 0 indicates successful convergence

# Optimized parameter estimates (back-transformed)
opt_param <- exp(mle_fit$par)
opt_param_names <- c("W_level", "V_obs")
names(opt_param) <- opt_param_names
print(opt_param)
# e.g., might output something like W_level ~ 0.05, V_obs ~ 2.3 (just as an example)

# Rebuild the DLM with estimated parameters
mod_fitted <- buildModel(mle_fit$par)
```

After MLE, `opt_param` contains the estimated variances. We then reconstruct `mod_fitted`, the DLM with these optimal values. At this stage, our model is fully specified. We have essentially **learned the variance hyperparameters** from data (a point estimate). If a fully Bayesian treatment is desired, we could set inverse-Gamma priors for these variances and use `dlmGibbsDIG` to sample their posterior distribution, but for brevity we proceed with the MLE estimates. The state estimation and forecasting that follow still use Bayesian updating (the Kalman filter), treating the variances as known.

### 3.3.4 Filtering and Smoothing (State Estimation)

With the fitted model, we perform Kalman filtering to estimate the latent states over time, and Kalman smoothing to refine those estimates using all data. Filtering yields one-step ahead forecasts and updated state estimates at each time, while smoothing provides retrospective estimates of states using future data as well.

```{r}
# Apply Kalman filter to the multivariate series
filter_res <- dlmFilter(Y, mod_fitted)

# Extract filtered one-step ahead forecasts and residuals (optional)
y_hat <- filter_res$f    # one-step ahead predicted observations (matrix: same dim as Y)
y_err <- filter_res$y - y_hat  # residuals (differences) wherever data is not NA

# Extract filtered state estimates
filtered_states_mean <- filter_res$m    # matrix of filtered state means (each row = time, col = state)
filtered_states_var  <- filter_res$U    # variance (upper triangular form; use dlmSvd2var to get cov matrices)
# Note: dlmFilter stores state variance in SVD form (U and D components). To get actual covariance at time n:
final_state_cov <- dlmSvd2var(filter_res$U.R, filter_res$D.R)  # covariance of final state

# Apply Rauch-Tung-Striebel smoother for retrospective state estimates
smooth_res <- dlmSmooth(filter_res)

smoothed_states_mean <- smooth_res$s    # matrix of smoothed state means (each row = time, col = state)
smoothed_states_var  <- smooth_res$U    # (in SVD form similarly)
```

After filtering (`dlmFilter`), the object `filter_res` contains the filtered estimates. In particular, `filter_res$m` is a matrix where each row gives the mean of the state vector at the end of that day *t* (after observing *Y\[t,*\]\*). The Kalman filter naturally handled missing observations: if SF or any station had NA on a given day, the filter simply relied on the state prediction (no update for that component). The `dlmSmooth` results combine forward and backward passes to provide smoothed states `smooth_res$s`, which are more accurate estimates of the true latent states using all available data.

**Interpretation Tip:** The first component of the state (Level) represents the **common temperature signal** (centered on SF). The seasonal components (next 4 states) represent the estimated seasonal cycle (Fourier coefficients). The remaining state components are the **station biases**. By examining `smoothed_states_mean`, one can see, for example, the bias states converging to values that reflect each station’s average offset relative to SF.

### 3.3.5 Forecasting 3-Day, 7-Day, and 14-Day Ahead

Finally, we use the fitted and filtered model to predict future temperatures. We will produce **short-term forecasts** for SF’s minimum and maximum temperature for the next 3, 7, and 14 days. The `dlmForecast` function provides the forecasted means and variances for any number of steps ahead. We focus on SF (first station), but the function actually forecasts all stations simultaneously (leveraging the state structure).

```{r}
# Forecast 14 days ahead using the filtered state as the starting point
n_ahead <- 14
forecast_res <- dlmForecast(filter_res, nAhead = n_ahead)

# Extract forecast means for all stations
Y_fore_mean <- forecast_res$f   # a list or matrix of forecast means (nAhead x n_stations)
# Extract forecast covariance matrices for each forecast step
Y_fore_cov <- forecast_res$Q    # list of length nAhead, each an (n_stations x n_stations) covariance matrix

# Focus on San Francisco (column 1)
sf_fore_mean <- if(is.list(Y_fore_mean)) {
  # dlmForecast prior to dlm v1.1 returns a list; newer versions return matrix
  do.call(rbind, Y_fore_mean)[,1]
} else {
  Y_fore_mean[,1]
}

# Compute prediction intervals for SF forecasts (e.g., 90% interval)
alpha <- 0.10  # for 90% interval (10% in lower tail, 10% in upper tail)
# Get standard deviation of SF forecast at each horizon
sf_fore_sd <- sapply(Y_fore_cov, function(Sig) sqrt(Sig[1,1]))
z <- qnorm(1 - alpha/2)  # z for central (1-alpha)% interval (e.g., 1.6449 for 90%)
sf_fore_lower <- sf_fore_mean - z * sf_fore_sd
sf_fore_upper <- sf_fore_mean + z * sf_fore_sd

# Print the 3-day, 7-day, 14-day ahead forecast for SF Max Temperature:
horizons <- c(3, 7, 14)
forecast_table <- data.frame(
  Horizon = horizons,
  Forecast_Date = as.Date(tail(tmax_wide$date, 1)) + horizons,  # last date + horizon
  SF_Max_Temp_Forecast = sf_fore_mean[horizons],
  Lower90 = sf_fore_lower[horizons],
  Upper90 = sf_fore_upper[horizons]
)
print(forecast_table)
```

This yields a table of SF’s forecasted max temperatures for the next 3, 7, and 14 days, along with 90% prediction intervals. (For minimum temperatures, we would repeat the modeling with `Y_min` or build a similar model for TMIN. Often TMIN and TMAX are modeled separately due to differing daily dynamics, but the procedure is analogous.)

**Forecast interpretation:** The model’s forecasts combine the last estimated state (which encodes the recent level of temperature and seasonality) with its dynamics. For example, if the last observed day was unusually warm, the **global level state** $L_t$ will be higher, carrying that signal into the forecast for subsequent days (gradually decaying if the model expects mean reversion via the level’s variance). The seasonal states ensure the forecast respects the time of year (e.g., rising trend into summer, or falling into winter). The **borrowed strength** from other stations means if SF had missing data or anomalous behavior, the model still anchors to the regional pattern gleaned from neighbors.

### 3.3.6 Plotting the Predictions

Finally, we plot the forecast results for visualization. We show the observed SF temperature series (recent period) and the 14-day forecast with its prediction interval.

```{r}
# Plot the last 60 days of observed SF data and the 14-day forecast
sf_actual <- Y[, 1]  # SF historical series (max temp)
date_index <- as.Date(tmax_wide$date)   # actual dates in the data
end_date <- tail(date_index, 1)
plot_window <- 60  # how many past days to show
plot_dates <- tail(date_index, plot_window)
plot_values <- tail(sf_actual, plot_window)

# Base plot for historical data
plot(plot_dates, plot_values, type="l", col="black", lwd=2,
     xlim = c(min(plot_dates), end_date + n_ahead),
     ylim = range(c(plot_values, sf_fore_lower, sf_fore_upper), na.rm=TRUE),
     xlab="Date", ylab="Temperature (°C)", 
     main="San Francisco Daily Max Temperature: Forecast for 14 Days Ahead")
grid()

# Add forecast mean line and confidence interval
fore_dates <- seq(end_date + 1, by = "day", length.out = n_ahead)
lines(fore_dates, sf_fore_mean, col="steelblue", lwd=2, lty=1)
# Shaded prediction interval (90%)
polygon(x = c(fore_dates, rev(fore_dates)),
        y = c(sf_fore_lower, rev(sf_fore_upper)),
        col = rgb(0, 0, 1, 0.2), border = NA)
lines(fore_dates, sf_fore_lower, col="steelblue", lty=2, lwd=1)
lines(fore_dates, sf_fore_upper, col="steelblue", lty=2, lwd=1)

# Add legend
legend("topleft", 
       legend = c("Observed SF Max Temp", "Forecast Mean", "90% Prediction Interval"), 
       col = c("black", "steelblue", "steelblue"), lty = c(1, 1, 2), lwd = c(2, 2, 1), bg = "white")
```

This plot illustrates the model’s fit and forecast. The black line shows the recent actual SF maximum temperatures. The blue line and band show the forecasted mean and the 90% confidence interval for the next 14 days. You can see the seasonal upward trend (if heading into summer) or downward trend (into winter) captured by the model, along with uncertainty that grows with the forecast horizon. The **other stations’ data** have informed the forecast by stabilizing the seasonal baseline and recent level: for instance, if SF had a sudden spike on the last day that was not seen in other stations, the filter might attribute part of it to transient noise, tempering the level state increase, whereas a region-wide heatwave would raise the level state more significantly.

#### 3.3.7 Conclusion

We have developed a comprehensive multistation DLM for forecasting San Francisco’s daily minimum and maximum temperatures. The model leverages **Bayesian state-space methods** (Kalman filtering/smoothing) to combine information across multiple stations and incorporate **seasonal structure** as described by Petris et al. (2009). Key steps included specifying the state components (trend, seasonal harmonics, station biases), handling missing data inherently in the state-space framework, and using MLE (or Bayesian sampling) to infer unknown variance parameters. The resulting forecasts provide short-term predictions with uncertainty bounds, which can be invaluable for planning and analysis. This approach can be further refined (e.g., adding more harmonics, separate models for TMIN and TMAX, or including an AR term for short-term weather fluctuations), but even this basic model demonstrates the power of **borrowing strength** from related time series in a state-space setting to improve forecasts for a particular location like San Francisco.

# Version 2

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

# Versione 1 SCARTATA

```{r}
#| label: Necessary packages
#| include: false

library(dlm)
library(forecast)

library(mvtnorm)

library(ggplot2)
library(ggfortify)
library(Matrix)

library(tidyverse) # includes (lubridate), (dplyr), (ggplot2), (tidyr), (tidyselect)
library(tinytex)
library(viridis)
library(gridExtra)
library(magrittr)
library(textab)
library(reshape2)
library(lubridate)

library(modeltime)
library(timetk)
library(timechange)

library(conflicted)
conflicts_prefer(dplyr::filter)
```

## 1. Load and process

```{r}
# 1a) Load the libraries we will need:
library(dlm)        # for Dynamic Linear Models
library(tidyverse)  # for data manipulation (dplyr, tidyr, ggplot2, etc.)
library(lubridate)  # for working with Dates

# 1b) Read (or re‐read) the GHCN CSV (you already had this, but for clarity):
ghcn <- read.csv("fp_data/ghcn.txt", header = TRUE)
colnames(ghcn) <- c(
  "station_id",
  "station_name",
  "latitude",
  "longitude",
  "elevation_m",
  "date",
  "tmin_tenthC",
  "tmax_tenthC",
  "tavg_tenthC",
  "prcp_tenthmm"
)

# 1c) Convert types, compute actual Celsius / mm, and drop the “_tenth” fields:
ghcn <- ghcn %>%
  mutate(
    date  = as_date(date),                   # convert YYYY-MM-DD → Date
    tmin  = tmin_tenthC / 10,                # tenths of °C → °C
    tmax  = tmax_tenthC / 10,                # tenths of °C → °C
    prcp  = prcp_tenthmm / 10                 # tenths of mm → mm
  ) %>%
  select(
    station_id, station_name, latitude, longitude, elevation_m,
    date, tmin, tmax, prcp
  )

ghcn <- subset(ghcn, date >= as.Date("1976-11-01"))

# 1d) Restrict to only the 13 stations of interest (as per your problem statement):
target_stations <- c(
  "KALISPELL GLACIER AP",
  "LITTLE ROCK",
  "MARTINSBURG E W VIRGINIA RGNL",
  "WILMINGTON INTL AP",
  "AUGUSTA STATE AP",
  "HOULTON AP",
  "PELLSTON RGNL AP",
  "MASON CITY MUNI AP",
  "PRESCOTT LOVE FLD",
  "SAN FRANCISCO DWTN",
  "LARAMIE AP",
  "BAKER CITY AP",
  "PASO ROBLES MUNI AP"
)

ghcn_sub <- ghcn %>%
  filter(station_name %in% target_stations)
```

```{r}
tmin_wide <- ghcn_sub %>%
  select(date, station_name, tmin) %>%
  pivot_wider(
    names_from  = station_name,
    values_from = tmin
  ) %>%
  arrange(date)
```

```{r}
tmax_wide <- ghcn_sub %>%
  select(date, station_name, tmax) %>%
  pivot_wider(
    names_from  = station_name,
    values_from = tmax
  ) %>%
  arrange(date)
```

```{r}
tmin_wide <- tmin_wide %>% mutate(date = as.Date(date))
tmax_wide <- tmax_wide %>% mutate(date = as.Date(date))
```

```{r}
common_dates <- intersect(tmin_wide$date, tmax_wide$date)
```

```{r}
# 1f) Now take the INTERSECTION of the two date‐sets so that TMIN and TMAX 

combined <- inner_join(
  tmin_wide,
  tmax_wide,
  by = "date",
  suffix = c("_tmin", "_tmax")
)

tmin_wide2 <- combined %>%
  select(date, ends_with("_tmin")) %>%
  rename_with(~ str_remove(.x, "_tmin"))

tmax_wide2 <- combined %>%
  select(date, ends_with("_tmax")) %>%
  rename_with(~ str_remove(.x, "_tmax"))
```

```{r}
# 1g) If you want to drop any rows where ANY station is missing, you can:
tmin_wide <- tmin_wide %>% drop_na()
tmax_wide <- tmax_wide %>% drop_na()

# 1h) Final result: we keep the date vector aside, then create two matrices:
dates <- tmin_wide$date
Y_tmin <- as.matrix( tmin_wide %>% select(-date) )
Y_tmax <- as.matrix( tmax_wide %>% select(-date) )

# Check dimensions:
dim(Y_tmin)   # [number_of_days x 13]
dim(Y_tmax)   # should match
stopifnot( dim(Y_tmin) == dim(Y_tmax) )
```

## 2. Model‐Spec: multistation + seasonality + common latent “trend”

### 2.1 A “shared trend” block

```{r}
# 2a) Build a dlm with a *single* state μ_t, observed by 13 identical “1”‐loadings.
buildCommonTrend <- function(par) {
  # par[1] = log(V_trend)   ; par[2] = log(W_trend)
  V_trend <- exp(par[1])   
  W_trend <- exp(par[2])
  
  # F_trend is a (13 x 1) column of 1's: each station sees the same μ_t
  F_trend <- matrix(1, nrow = 13, ncol = 1)
  
  # G_trend: since it's a local‐level (order=1), G = 1 of dimension 1×1
  G_trend <- matrix(1, nrow = 1, ncol = 1)
  
  # V: Observation covariance for the *common trend block*.  Here we
  # will not set observation noise inside this block—because we'll let
  # each station have its own observation‐variance later.  To signal that
  # this “block” contributes no extra V, we set V_trend_block = 0 (a 13×13 zero)
  V_block <- diag(13) * 0
  
  # W: 1×1 variance of μ_t.  (It's a scalar in the 1‐dim block.)
  W_block <- matrix(W_trend, nrow = 1, ncol = 1)
  
  
  
  # Build the “dlm” object for just the common trend:
  dlm_trend <- dlm(
    FF   = F_trend,       # 13×1
    V    = V_block,       # 13×13 (zero)
    GG   = G_trend,       # 1×1
    W    = W_block,       # 1×1
    m0   = 0,             # prior mean of μ_0
    C0   = 1e7            # very large prior variance for μ_0 (effectively “flat”)
  )
  
  # dlm also needs to know the names/dimensions properly:
  return(dlm_trend)
}
```

### 2.2 A “station‐wise seasonal” block

```{r}
# 2b) Build a "2‐dim seasonal" block for a single station:
buildOneStationSeasonal <- function(par) {
  # par[1], par[2] = log(σ²_season_j1), log(σ²_season_j2)
  W1 <- exp(par[1])
  W2 <- exp(par[2])
  
  # G_seas_j: identity 2×2  (random‐walk evolution for the two Fourier coefficients)
  G_seas_j <- diag(2)
  
  # W_seas_j: diag(W1, W2)
  W_seas_j <- diag(c(W1, W2))
  
  # We do not place any V‐noise here (will be in the "obs noise" block).
  V_seas_j <- matrix(0, nrow = 2, ncol = 2)
  
  # Because this is a “sub‐DLM,” but we only build it to inspect G & W.
  # We will not feed this directly to dlm—rather, we will pick off G and W.
  # Instead, let’s return the two matrices so we can bdiag them later.

  return(
    list(
      G_j = G_seas_j,
      W_j = W_seas_j
    )
  )
}
```

```{r}
# 2c) Wrapper: Given a length‐26 log‐variance‐vector 'par_seas' = length 2*13
#     (i.e. for each station j, we have two log(σ²) parameters), build the
#     26×26 G_block and 26×26 W_block by block‐diagonal stacking:

buildAllStationsSeasonal <- function(par_seas) {
  # Expect length(par_seas) = 2 × 13 = 26
  if(length(par_seas) != 26) stop("par_seas must be length 26.")
  
  G_blocks <- list()
  W_blocks <- list()
  idx <- 1
  for(j in seq_len(13)) {
    # par_seas[c(idx,(idx+1))] are the two logs for station j
    subset <- par_seas[c(idx, idx+1)]
    tmp   <- buildOneStationSeasonal(subset)
    
    G_blocks[[j]] <- tmp$G_j  # 2×2
    W_blocks[[j]] <- tmp$W_j  # 2×2
    idx <- idx + 2
  }
  # Build block‐diagonal matrices of dimension (2*13)×(2*13):
  G_seas_all <- do.call(bdiag, G_blocks)
  W_seas_all <- do.call(bdiag, W_blocks)
  
  return(list(G_all = G_seas_all,
              W_all = W_seas_all))
}
```

### 2.3 The “station‐specific observation‐noise” block

```{r}
# 2d) Given a 13‐vector par_obs = log(σ²_obs₁), …, log(σ²_obs₁₃),
#     build the 13×13 diagonal V matrix:

buildObsVariance <- function(par_obs) {
  if(length(par_obs) != 13) stop("par_obs must be length 13.")
  Vdiag <- exp(par_obs)
  return(diag(Vdiag))
}
```

### 2.4 Putting all blocks together

```{r}
# 2e) Master “buildDLM” function.  Input: par_full (length=41), in order:
#        par_full[1:2]   = log(V_trend), log(W_trend)   (but we set V_trend=0)
#        par_full[3:28]  = 26 logs of seasonal‐evolution variances (2×13)
#        par_full[29:41] = 13 logs of station‐obs variances
#      Output: a single dlm object with 
#        F = 13×27, V=13×13 diag, G=27×27, W=27×27 diag.

buildDLM_allstations <- function(par_full, dates) {
  if(length(par_full) != 41) stop("par_full must be length 41.")
  
  # === Unpack: ===
  # 1-2: for μₜ block
  par_trend <- par_full[1:2]   # (we only need par_trend[2] for W; par_trend[1] is log(V_trend)=−Inf)
  # 3:28: for 2‐dim seasonal block, *per station*
  par_seas  <- par_full[3:28]
  # 29:41: for station observation variances
  par_obs   <- par_full[29:41]
  
  # --- (i) Build the common‐trend sub‐dlm: ---
  dlm_trend <- buildCommonTrend(par_trend)
  #   This gives us dlm_trend$FF (13×1), dlm_trend$GG (1×1), dlm_trend$W (1×1).
  
  # --- (ii) Build the seasonal sub‐blocks: ---
  tmp_seas <- buildAllStationsSeasonal(par_seas)
  G_seas_all <- tmp_seas$G_all   # 26×26
  W_seas_all <- tmp_seas$W_all   # 26×26
  
  # --- (iii) Build the overall G_big and W_big by block‐diagonal stacking: ---
  G_big <- bdiag( dlm_trend$GG, G_seas_all )    # (1 + 26) × (1 + 26) = 27×27
  W_big <- bdiag( dlm_trend$W,  W_seas_all )    # likewise 27×27
  
  # --- (iv) Build F_big (13×27): ---
  #  F_big = [ 1_{13×1} | seasonal_basis_matrix ]
  #  where seasonal_basis_matrix is 13 columns picking out the TWO seasonal states 
  #  for each station, but we also need to plug in cos(2πt/365), sin(2πt/365) 
  #  at each day t.  For each station j (j=1..13), we do:
  #    row j: F_big[j, 1] = 1   (common μ_t)
  #           F_big[j, (1 + (2*(j-1)+1)) ] = cos(2π t/365)
  #           F_big[j, (1 + (2*(j-1)+2)) ] = sin(2π t/365)
  #
  #  Actually: careful: each row j corresponds to station j.  The columns of α_t are
  #    [μ_t,  β_{1,cos,t}, β_{1,sin,t}, β_{2,cos,t}, β_{2,sin,t}, …, β_{13,cos,t}, β_{13,sin,t}].
  #  So “station j’s seasonal states” are in α_t at indices (2*(j-1)+2) and (2*(j-1)+3),
  #  if we count the very first element as index=1 (for μ_t).  Concretely:
  
  # Number of days total:
  n_days <- length(dates)
  # But F_big changes with time (because cos(2πt/365) depends on actual t).  
  # So we will build a time‐varying F: a 3‐dim array of dims (13 × 27 × n_days).
  F_big_array <- array(0, dim = c(13, 27, n_days))
  
  for(tt in seq_len(n_days)) {
    tval <- as.numeric(dates[tt] - min(dates))  # “day‐index” starting at 0
    omega <- 2 * pi * (tval %% 365) / 365        # within‐year position
    
    # For each station j:
    for(j in seq_len(13)) {
      # 1) common‐trend loading:
      F_big_array[j, 1, tt] <- 1  
      
      # 2) station‐j seasonal pair (cos, sin):
      cos_j <- cos(omega)
      sin_j <- sin(omega)
      # The seasonal states for station j occupy indices = 1 (for mu) + [2*(j-1)+1, 2*(j-1)+2]:
      idx1 <- 1 + (2*(j-1) + 1)
      idx2 <- 1 + (2*(j-1) + 2)
      F_big_array[j, idx1, tt] <- cos_j
      F_big_array[j, idx2, tt] <- sin_j
    }
  }
  
  # --- (v) Build V_big (13×13 diag of obs variances): ---
  V_big <- buildObsVariance(par_obs)  # diag(exp(par_obs))
  
  # --- (vi) Prior mean m0 and covariance C0 for α₀ (state at time=0) ---
  # Let’s give μ₀ ~ N(0, 10⁷) and every seasonal coefficient ~ N(0, 10⁴):
  m0_big <- rep(0, 27)
  C0_big <- diag( c(1e7, rep(1e4, 26)) )
  
  # Finally, put everything into one dlm object.
  mod <- dlm(
    FF   = F_big_array,   # time‐varying: 13×27×n_days
    V    = V_big,         # 13×13
    GG   = as.matrix(G_big), # 27×27
    W    = as.matrix(W_big), # 27×27
    m0   = m0_big,          
    C0   = C0_big
  )
  return(mod)
}
```

## 3. Estimating the unknown variances via dlmMLE

```{r}
# 3a) Initial guesses for the 41 logs:
init_par <- numeric(41)
init_par[1] <- log(1e-12)  # effectively fix V_trend ≈ 0
init_par[2] <- log(0.1^2)  # W_trend ~ 0.01
init_par[3:28] <- log(rep(0.01^2, 13*2))  # W_seas_j1 ~ 0.0001, W_seas_j2 ~ 0.0001
init_par[29:41] <- log(rep(2^2, 13))  # σ_obs_j = 2°C  → var = 4

# 3b) Define the “objective” function to minimize (−logLik):
dlm_nLL <- function(par) {
  # Build the DLM
  mod <- buildDLM_allstations(par, dates)
  # Run the Kalman filter
  ff  <- dlmFilter(Y_tmin, mod)
  # -log‐likelihood is reported by dlmLL
  return(-dlmLL(ff))
}

conflicts_prefer(Matrix::bdiag)

# 3c) Optimize via dlmMLE
fit_tmin <- dlmMLE(
  y      = Y_tmin, 
  parm   = init_par, 
  build  = function(param) buildDLM_allstations(param, dates),
  lower  = rep(log(1e-6), 41),  # keep all variances ≥ 1e-6
  upper  = rep(log(1e2),  41),  # keep all variances ≤ 1e2
  control = list(maxit = 200)
)

conflicts_prefer(dlm::bdiag)

fit_tmin$convergence
# if 0, good. fit_tmin$par is the MLE of the 41 logs.

# 3d) Build the *fitted* model for TMIN:
mod_tmin_fitted <- buildDLM_allstations(fit_tmin$par, dates)

# Do the exact same steps for TMAX:
dlm_nLL_max <- function(par) {
  mod <- buildDLM_allstations(par, dates)
  ff  <- dlmFilter(Y_tmax, mod)
  return(-dlmLL(ff))
}

fit_tmax <- dlmMLE(
  y      = Y_tmax,
  parm   = init_par,
  build  = function(param) buildDLM_allstations(param, dates),
  lower  = rep(log(1e-6), 41),
  upper  = rep(log(1e2),  41),
  control = list(maxit = 200)
)
mod_tmax_fitted <- buildDLM_allstations(fit_tmax$par, dates)
```

## 4. Filtering, smoothing, and one‐step forecast

```{r}
# 4a) Kalman filter on TMIN:
filt_tmin <- dlmFilter(Y_tmin, mod_tmin_fitted)

# 4b) Extract the one‐step‐ahead forecasts for day t: 
#      a_t = E(α_t | y_{1:(t−1)}) 
#      f_t = E(Y_t | y_{1:(t−1)})   i.e. the (13-dimensional) one‐step forecast 
one_step_forecasts_tmin <- dropFirst(filt_tmin$f)  
#   dims: [N × 13], where row t = forecasted Y_t at time t, using data up to t−1.

# 4c) Filtering residuals (innovations): 
#      v_t = Y_t − f_t 
innovations_tmin <- residuals(filt_tmin, type = "raw")

# 4d) Now run the smoother, to get α̂_{t|T} for all t = 1..N:
smooth_tmin <- dlmSmooth(filt_tmin)

# 4e) The smoothed state matrix (N+1 × 27).  The first row is α̂_{0|T} (we ignore),
#      the second row is α̂_{1|T}, etc., up to α̂_{N|T}.  So drop the first:
alpha_sm_tmin <- smooth_tmin$s[-1, ]   # a (N × 27) matrix.

# 4f) Extract the **common trend** μ̂_t from the smoothed states:
#      that's column 1 of alpha_sm_tmin:
mu_hat_tmin <- alpha_sm_tmin[, 1]      # length N

# 4g) Extract each station’s seasonal estimate β_{j,cos,t}, β_{j,sin,t}:
seasonal_hat_tmin <- list()
for(j in seq_len(13)) {
  idx1 <- (2*(j-1) + 2)
  idx2 <- (2*(j-1) + 3)
  seasonal_hat_tmin[[j]] <- alpha_sm_tmin[, c(idx1, idx2)] %*%
    cbind(cos(2*pi * ((as.numeric(dates - min(dates))) %% 365)/365),
          sin(2*pi * ((as.numeric(dates - min(dates))) %% 365)/365)
         )
  # This gives actual seasonal reconstruction s_{j,t} = [β_{j,cos,t}  β_{j,sin,t}] ⋅ [cos; sin].
}

# 4h) One‐day‐ahead forecast for TMIN at the last time:
#     We can use dlmForecast() or else the built‐in predict function.
fcst_tmin_1step <- dlmForecast(
  filt_tmin,
  nAhead = 1
)
# fcst_tmin_1step$f is a (1 × 13) vector of E(Y_{N+1} | data up to day N).
# fcst_tmin_1step$Q is the (1 × 1 list containing) covariance matrix of forecast errors.

# 4i) Similarly, if you want a 7‐day forecast:
fcst_tmin_7 <- dlmForecast(filt_tmin, nAhead = 7)
# which gives 7 rows of (13‐dim) forecasts.  
```

```{r}
filt_tmax <- dlmFilter(Y_tmax, mod_tmax_fitted)
smooth_tmax <- dlmSmooth(filt_tmax)
alpha_sm_tmax <- smooth_tmax$s[-1, ]    # (N × 27)
mu_hat_tmax <- alpha_sm_tmax[,1]
# station‐wise seasonal reconstructions:
seasonal_hat_tmax <- list()
for(j in seq_len(13)) {
  idx1 <- (2*(j-1) + 2); idx2 <- (2*(j-1) + 3)
  seasonal_hat_tmax[[j]] <- alpha_sm_tmax[, c(idx1, idx2)] %*%
    cbind(cos(2*pi * ((as.numeric(dates - min(dates))) %% 365)/365),
          sin(2*pi * ((as.numeric(dates - min(dates))) %% 365)/365)
         )
}
# One‐step‐ahead forecast for TMAX tomorrow:
fcst_tmax_1step <- dlmForecast(filt_tmax, nAhead = 1)
```

## 5. Inspecting “borrowing strength” & the common latent trend for SF

```{r}
# 5a) Identify which column is “San Francisco” in your 13 columns:
station_names <- colnames(tmin_wide)[-1]   # this is a character vector of length 13
station_names
# Suppose "SAN FRANCISCO DWTN" is column number k_sf:
k_sf <- which(station_names == "SAN FRANCISCO DWTN")

# 5b) Extract SF’s smoothed series vs. SF’s smoothed seasonal + common trend:
sf_smoothed_trend <- mu_hat_tmin  # common trend
sf_smoothed_season <- seasonal_hat_tmin[[k_sf]]
sf_combined_smoothed <- sf_smoothed_trend + sf_smoothed_season

# 5c) Plot observed vs. smoothed:
df_plot <- tibble(
  date = dates,
  observed_sf_tmin   = Y_tmin[, k_sf],
  common_trend       = mu_hat_tmin,
  sf_seasonal        = sf_smoothed_season,
  sf_combined        = sf_combined_smoothed
)

library(ggplot2)
ggplot(df_plot, aes(x = date)) +
  geom_line(aes(y = observed_sf_tmin),    color = "steelblue") +
  geom_line(aes(y = common_trend),        color = "orange", linetype = "dashed") +
  geom_line(aes(y = sf_combined),         color = "forestgreen",  linetype = "solid") +
  labs(
    title = "San Francisco TMIN: Observed vs. Common Trend vs. Trend+Season",
    y = "Temperature (°C)"
  ) +
  theme_minimal()

```

## 6. Making short‐term predictions for SF

```{r}
# 6a) One‐step forecast (TMIN) for tomorrow in SF:
fcst1_tmin_sf <- fcst_tmin_1step$f[1, k_sf]   # point forecast
fcst1_tmin_sf_se <- sqrt(fcst_tmin_1step$Q[,,1][k_sf, k_sf])  # st. dev

# 6b) 7‐day forecast (TMIN) for SF:
fcst7_tmin_sf <- fcst_tmin_7$f[ , k_sf]      # 7 rows, one per day
# The corresponding Q is an array; you can extract diagonals for each day as se’s.
```

```{r}
fcst1_tmax_sf <- fcst_tmax_1step$f[1, k_sf]
fcst1_tmax_sf_se <- sqrt(fcst_tmax_1step$Q[,,1][k_sf, k_sf])
```
