---
title: "20236_FinalProject"
author: "Stefano Graziosi, Gabriele Molè, Laura Lo Schiavo, Giovanni Carron"
format: pdf
editor: visual
---

```{r}
#| label: Necessary packages
#| include: false

library(dlm)
library(forecast)

library(mvtnorm)

library(ggplot2)
library(ggfortify)

library(tidyverse) # includes (lubridate), (dplyr), (ggplot2), (tidyr), (tidyselect)
library(tinytex)
library(viridis)
library(gridExtra)
library(magrittr)
library(textab)
library(reshape2)

library(modeltime)
library(timetk)
library(timechange)

library(conflicted)
conflicts_prefer(dplyr::filter)
```

# Description of the problem

This project explores both short and long term climatological patterns using data from the Global Historical Climatology Network (GHCN) database. The GHCN database represents one of the most comprehensive and reliable sources of historical climate and weather data, making it particularly valuable for studying temporal patterns.

A fundamental distinction in atmospheric science lies between weather and climate analysis. Weather refers to short-term variations in temperature, precipitation, and other meteorological variables. These short-term fluctuations can be highly variable and are typically studied for day-to-day (or week-to-week) forecasting. Climate, in contrast, represents the long-term patterns and averages of these weather conditions, usually analyzed over periods of decades. We display in Figure 1 and Figure 2 two examples of climate and weather data, respectively.

Through this project you will investigate both phenomena exploiting different statistical methods you will learn during the course. For instance, some of the questions you will be asked to answer are:

1.  Can we identify any long-term trends in temperature data?

2.  Can we exploit recent data to forecast temperature of the following temporal steps?

While for weather purposes you can directly work with raw data from the GHCN archive, in order to extract some long term information it is better to analyze aggregated data. In fact, some pre--processing steps stress macro--behaviors by smoothing out some short-term variations. In this direction, one of the most reliable and influential sources is GISTEMP (GISS Surface Temperature Analysis) a product of the NASA Goddard Institute for Space Studies which merges some data gathered from GHCN meteorological stations with some ocean related data from ERSST stations.

# Data description

## GISTEMP

As a first step, you will focus on the temperature data coming from GISTEMP. The series provided represents changes in the global surface temperature over time. It is derived from a global network of land and ship weather stations, which track daily surface temperatures. Measurements are then adjusted for urban heat effects and other biases, aggregated monthly and averaged across stations. Lastly, since the quantity of interest is the variation of the global surface temperature, the final measurements are adjusted by subtracting the average global surface temperature over the period 1951-1980, which serves as a reference value.

The data set provides a reliable long-term record of temperature anomalies, offering valuable information on climate trends and variability.

The data is provided in a csv file named `gistemp.txt`. Each row refers to a calendar year (starting from 1880, up to 2024) and contains the following variables.

-   **1st column**: calendar year;
-   **2nd to 13th**: monthly temperature difference with respect to reference period;
-   **14th to 15th**: annual average of temperature difference taken as Jan-to-Dec (`J-D`) or Dec-to-Nov (`D-N`)
-   **16th to 19th**: seasonal average for winter (`DJF`), spring (`MAM`), summer (`JJA`) and autumn (`SON`).

You will need to extract the relevant variable (the monthly data) and convert it to a properly formatted ts object.

## GHCN

Next, we turn our attention to the GHCN (Global Historical Climatology Network) dataset, which provides high-resolution daily climate observations from thousands of land-based weather stations around the world. The data is widely used for studies on local and regional climate patterns, extreme weather events, and short-term trends.

The dataset includes daily measurements of key meteorological variables such as temperature and precipitation. Each observation records minimum, maximum, and average daily temperatures, as well as the amount of precipitation, making it particularly valuable for fine-grained temporal analyses.

The data is provided in a .`txt` file named `ghcn.txt`. Each row in the dataset corresponds to a single daily observation from a specific weather station. The columns are as follows:

-   **1st column**: Station ID (a unique identifier for each weather station);

-   **2nd column**: Station name;

-   **3rd to 5th columns**: Geographic coordinates and elevation of the station (latitude, longitude, elevation in meters);

-   **6th column**: Date of observation (formatted as YYYY-MM-DD).

-   **7th column**: Minimum temperature of the day (TMIN), recorded in tenths of degrees Celsius;

-   **8th column**: Maximum temperature of the day (TMAX), recorded in tenths of degrees Celsius;

-   **9th column**: Average temperature of the day (TAVG), recorded in tenths of degrees Celsius;

-   **10th column**: Daily total precipitation (PRCP), recorded in tenths of millimeters.

# Task 1 \| Data acquisition and exploration

Extract the data from the GISTEMP and GHCN datasets. Specifically, for daily data we will focus solely maximum and minimum temperature measurements from the `San Francisco Downtown` station. Describe suitably the two time series, with appropriate plots and comments. Perform a time series decomposition using appropriate tools and highlight relevant features (if present) for each component.

```{r}
#| label: Load GISTEMP

gistemp <- read.csv("fp_data/gistemp.txt", header = TRUE) 
monthly_gistemp <- gistemp[, 2:13] 
ts_gistemp <- ts(as.vector(t(monthly_gistemp)),
                 start = c(1880, 1),
                 frequency = 12)
```

Keep in mind, our model will include a seasonal component, hence this chunk **shall not be run**

```{r}
#| label: Adjusting for seasonality
stl_decomp <- stl(ts_gistemp, s.window = "periodic", robust = TRUE)

seasonal_comp <- stl_decomp$time.series[, "seasonal"]
ts_gistemp_ds <- ts_gistemp - seasonal_comp

```

```{r}
#| label: Load GHCN

ghcn <- read.csv("fp_data/ghcn.txt", header = TRUE) 

colnames(ghcn) <- c(
  "station_id",
  "station_name",
  "latitude",
  "longitude",
  "elevation_m",
  "date",
  "tmin_tenthC",
  "tmax_tenthC",
  "tavg_tenthC",
  "prcp_tenthmm"
)

ghcn <- ghcn %>%
  mutate(
    date  = as_date(date),                   # YYYY-MM-DD → Date
    tmin  = tmin_tenthC / 10,                # → °C
    tmax  = tmax_tenthC / 10,                # → °C
    prcp  = prcp_tenthmm / 10                # → mm
  ) %>%
  select(-tmin_tenthC, -tmax_tenthC, -tavg_tenthC, -prcp_tenthmm)
```

```{r}
#| label: Filtering for the San Francisco Station

sf <- ghcn %>%
  filter(station_id == "USW00023272") %>%
  arrange(date)
```

```{r}
#| label: Converting to a daily time series object

start_yr  <- year(min(sf$date))
start_doy <- yday(min(sf$date))

ts_tmin <- ts(sf$tmin,
              start     = c(start_yr, start_doy),
              frequency = 365)
ts_tmax <- ts(sf$tmax,
              start     = c(start_yr, start_doy),
              frequency = 365)
ts_prcp <- ts(sf$prcp,
              start     = c(start_yr, start_doy),
              frequency = 365)
```

# Task 2 \| GISTEMP: Do the data document global warming?

Consider the seasonality adjusted time-series from the GISTEMP data.

## 2.1 Hidden Markov Models

Explore the use of Hidden Markov Models to identify the presence of long term temperature trends and/or change points inside the data. Comment on the results, highlighting advantages and limitations of the approach.

### Inizio codice HMM

# 1. Specify the HMM with 2 states

hmm_2 \<- depmix(ts_gistemp_ds \~ 1, family = gaussian(), nstates = 2, data = data.frame(ts_gistemp_ds))

# 2. Fit the model

set.seed(123) fmodel \<- fit(hmm_2, verbose = FALSE)

# 3. Extract estimated mean and standard deviation for ALL hidden states

fmodel\@response\[\[1\]\]\[\[1\]\]@parameters$coefficients fmodel\@response\[\[1\]\]\[\[1\]\]@parameters$sd fmodel\@response\[\[2\]\]\[\[1\]\]@parameters$coefficients fmodel\@response\[\[2\]\]\[\[1\]\]@parameters$sd

# 4. Compute standard errors

MLEse \<- standardError(fmodel) round(MLEse\$par,3)

# 1. Specify the HMM with 3 states

hmm_3 \<- depmix(ts_gistemp_ds \~ 1, family = gaussian(), nstates = 3, data = data.frame(ts_gistemp_ds))

# 2. Fit the model

set.seed(123) fmodel3 \<- fit(hmm_3, verbose = FALSE)

fmodel3\@response\[\[1\]\]\[\[1\]\]@parameters$coefficients fmodel3\@response\[\[1\]\]\[\[1\]\]@parameters$sd fmodel3\@response\[\[2\]\]\[\[1\]\]@parameters$coefficients fmodel3\@response\[\[2\]\]\[\[1\]\]@parameters$sd fmodel3\@response\[\[3\]\]\[\[1\]\]@parameters$coefficients fmodel3\@response\[\[3\]\]\[\[1\]\]@parameters$sd

# 4. Compute standard errors

MLEse3 \<- standardError(fmodel3) round(MLEse3\$par,3)

#Get the estimated state for each timestep estStates \<- posterior(fmodel)

plot(time(ts_gistemp_ds), estStates\$state, type = "l", ylab = "Estimated State", xlab = "Time", main = "Estimated Hidden States over Time (2-state HMM)", col = "darkred", lwd = 1.5)

# Step 1: Get estimated states

estStates \<- posterior(fmodel)

# Step 2: Assign state numbers to i and ii

i \<- estStates\$state\[1\] ii \<- if (i == 1) 2 else 1

# Step 3: Get the estimated means for both states

estMean1 \<- fmodel\@response\[\[i\]\]\[\[1\]\]@parameters$coefficients estMean2 \<- fmodel\@response\[\[ii\]\]\[\[1\]\]@parameters$coefficients

# Step 4: Create a vector of means based on state assignments

estMeans \<- rep(estMean1, length(ts_gistemp_ds)) estMeans\[estStates\$state == ii\] \<- estMean2

# Step 5: Plot original series and estimated means

plot(ts_gistemp_ds, main = "Deseasonalized Data and HMM Estimated Means", ylab = "Temperature Anomaly", xlab = "Time", col = "gray")

# Add estimated means as points

points(time(ts_gistemp_ds), estMeans, col = "blue", cex = 0.3)

library(dplyr) library(tidyr) library(ggplot2)

# Crea un data frame con la serie e lo stato stimato

results_df \<- data.frame( time_index = as.numeric(time(ts_gistemp_ds)), sample_trajectory = as.numeric(ts_gistemp_ds), estimated_state = estStates\$state ) %\>% pivot_longer(cols = c("sample_trajectory", "estimated_state"), names_to = "variable", values_to = "value")

# Crea il grafico

plotobj \<- ggplot(results_df, aes(x = time_index, y = value)) + geom_line() + facet_wrap(\~variable, scales = "free", ncol = 1) + theme_minimal() + labs(title = "HMM States and Deseasonalized Series", x = "Time", y = "Value")

# Mostra il grafico

plot(plotobj)

# Load packages

library(ggplot2) library(dplyr)

# Get time and values

time_vals \<- as.numeric(time(ts_gistemp_ds)) values \<- as.numeric(ts_gistemp_ds) states \<- posterior(fmodel)\$state

# Create data frame

df \<- data.frame( time = time_vals, value = values, state = factor(states) )

# Optional: Fit regression models by state

model1 \<- lm(value \~ time, data = df %\>% filter(state == 1)) model2 \<- lm(value \~ time, data = df %\>% filter(state == 2))

# Create plot with confidence intervals

p \<- ggplot(df, aes(x = time, y = value, color = state, fill = state)) + geom_point(alpha = 0.4, size = 0.5) + geom_smooth(method = "loess", se = TRUE, linetype = "solid", linewidth = 1.2, alpha = 0.2) + labs(title = "Linear Trends by Hidden State (HMM) with Confidence Intervals", x = "Time", y = "Deseasonalized Temperature Anomaly") + theme_minimal() + scale_color_manual(values = c("steelblue", "darkorange")) + scale_fill_manual(values = c("steelblue", "darkorange"))

# Show plot

print(p)

# Prepare the time variable

time_vals \<- as.numeric(time(ts_gistemp_ds))

# Combine time and deseasoned series into a dataframe

df \<- data.frame( temp = as.numeric(ts_gistemp_ds), time = time_vals )

# Specify HMM with linear trend in each state: response \~ time

hmm_linear \<- depmix(temp \~ time, family = gaussian(), nstates = 2, data = df)

# Fit the model

set.seed(123) fmodel_linear \<- fit(hmm_linear, verbose = FALSE)

# Extract estimated intercepts and slopes

coef1 \<- fmodel_linear\@response\[\[1\]\]\[\[1\]\]@parameters$coefficients \# State 1 coef2 \<- fmodel_linear\@response\[\[2\]\]\[\[1\]\]@parameters$coefficients \# State 2

# Optional: view

coef1 \# (Intercept and Slope for State 1) coef2 \# (Intercept and Slope for State 2)

# Get posterior state assignments

estStates \<- posterior(fmodel_linear)

# Compute fitted values using state-specific regression

fitted_vals \<- numeric(length(df$temp)) fitted_vals[estStates$state == 1\] \<- coef1\[1\] + coef1\[2\] \* df$time[estStates$state == 1\] fitted_vals\[estStates$state == 2] <- coef2[1] + coef2[2] * df$time\[estStates\$state == 2\]

# Plot original series with fitted trend lines

plot(df$time, df$temp, main = "Deseasonalized Data with State-Dependent Linear Trends", ylab = "Temperature Anomaly", xlab = "Time", type = "l", col = "gray")

lines(df\$time, fitted_vals, col = "blue", lwd = 2)

# Prepare the time variable

time_vals \<- as.numeric(time(ts_gistemp_ds))

# Combine time and deseasoned series into a dataframe

df \<- data.frame( temp = as.numeric(ts_gistemp_ds), time = time_vals )

# Specify HMM with linear trend in each state: response \~ time

hmm_linear3 \<- depmix(temp \~ time, family = gaussian(), nstates = 3, data = df)

# Fit the model

set.seed(123) fmodel_linear3 \<- fit(hmm_linear3, verbose = FALSE)

# Extract estimated intercepts and slopes

coef1 \<- fmodel_linear3\@response\[\[1\]\]\[\[1\]\]@parameters$coefficients \# State 1 coef2 \<- fmodel_linear3\@response\[\[2\]\]\[\[1\]\]@parameters$coefficients \# State 2 coef3 \<- fmodel_linear3\@response\[\[3\]\]\[\[1\]\]@parameters$coefficients \# State 3

# Optional: view

coef1 \# (Intercept and Slope for State 1) coef2 \# (Intercept and Slope for State 2) coef3 \# (Intercept and Slope for State 3)

## 2.2 Dynamic Linear Models

Explore the use of Dynamic Linear Models such as random walk plus noise or locally linear trend to address the presence of acceleration/deceleration in global warming. You can opt to use more structured models (if you choose a model with seasonality, you can use the full time series). Comment on the results, and compare them with the ones obtained using HMMs.

> Stefano:

```{r}
#| label: Building the DLM

# 1. Builder: log‐parameters = log(c(V, W_mu, W_beta, W_seas))
buildLL2S <- function(par) {
  V      <- exp(par[1])
  W_mu   <- exp(par[2])
  W_beta <- exp(par[3])
  W_seas <- exp(par[4])
  trend  <- dlmModPoly(order = 2,
                       dV = V,
                       dW = c(W_mu, W_beta))
  seas   <- dlmModSeas(frequency = 12, 
                       dV = 0,
                       dW = c(W_seas, rep(0,10)))
  trend + seas
}
```

```{r}
#| label: Obtaining the MLE estimates

init <- log(c(var(ts_gistemp),  # V
              var(ts_gistemp)/100,  # W_mu
              var(ts_gistemp)/1000, # W_beta
              var(ts_gistemp)/100)) # W_seas

fitLL2S <- dlmMLE(ts_gistemp, parm = init, build = buildLL2S)
modLL2S <- buildLL2S(fitLL2S$par)
```

```{r}
#| label: Filtering and smoothing
#| fig-width: 12
#| fig-height: 6

filtLL2S <- dlmFilter(ts_gistemp, modLL2S)
smthLL2S <- dlmSmooth(filtLL2S)

# 2. Extract the smoothed state means (drop the t=0 init)
m_smth_all <- smthLL2S$s[-1, ]    # matrix with columns = (level, slope, seasonal…)
level_smth <- m_smth_all[, 1]     # the level μ_t

# 3. Compute the smoothing covariances → standard deviations
Ct_smooth_list <- dlmSvd2var(smthLL2S$U.S, smthLL2S$D.S)
var_level      <- sapply(Ct_smooth_list, function(mat) mat[1,1])  # length T+1
sd_level       <- sqrt(var_level)[-1]                             # drop init → length T

# 4. Build 95% credible intervals via Forward Filtering Backward Sampling (FFBS)
  # 4.a.Running the Kalman filter

modFilt <- dlmFilter(ts_gistemp, modLL2S)

  # 4.b. backward‐sample many smooth draws of the _level_ (state component 1)
set.seed(123)
n.sims <- 1000

sims.level <- replicate(
  n.sims,
  {
    full.draw <- dlmBSample(modFilt)       # one draw
    as.numeric(full.draw[-1, 1])           # drop init, take level                                        QUI IL TOSTAPANE INIZIA A SCALDARSI
  }
)

# 4.c. empirical 95% bands
lower95_sim <- apply(sims.level, 1, quantile, probs = 0.025)
upper95_sim <- apply(sims.level, 1, quantile, probs = 0.975)

# 5. Time index (same as before)
time_idx <- seq(
                from = start(ts_gistemp)[1] + (start(ts_gistemp)[2]-1)/12,
                by   = 1/12,
                length.out = length(ts_gistemp)
)

df_smth <- data.frame(
                      Time       = time_idx,
                      Anomaly    = as.numeric(ts_gistemp),
                      Level_smth = level_smth,
                      Upper95    = upper95_sim,
                      Lower95    = lower95_sim
)

# 6. Plot
ggplot(df_smth, aes(x = Time)) +
  geom_line(aes(y = Anomaly),  color = "orange", linewidth = 0.7) +
  geom_line(aes(y = Level_smth), color = "steelblue", linewidth = 0.5) +
  geom_ribbon(aes(ymin = Lower95, ymax = Upper95),
              fill = "steelblue", alpha = 0.2) +
  labs(
    title    = "Smoothed Level from Seasonally-Augmented Local Linear Trend",
    subtitle = "95% credible intervals around the smoothed μ_t",
    x        = "Year",
    y        = "Temperature Anomaly (°C)"
  ) +
  theme_minimal() +
  theme(
    plot.title    = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12, face = "italic")
  )

```

### 2.2.a Evaluating the model

```{r}
#| label: Residual diagnostics

# (a) raw and standardized one‐step–ahead errors
raw_resid <- residuals(filtLL2S, type = "raw")
std_resid <- residuals(filtLL2S, type = "standardized", sd = FALSE)

# (b) Plot standardized residuals over time
ts.plot(std_resid, ylab = "Std. Residuals", main = "Standardized One-step-Ahead Errors")
abline(h = c(-2,2), col = "firebrick", lty = 2)

# (c) ACF to check for remaining autocorrelation
acf(std_resid, main = "ACF of Standardized Residuals")

# (d) Histogram + normal curve
hist(std_resid, freq = FALSE, main = "Histogram of Std. Residuals",
     xlab = "Std. Residual")
curve(dnorm(x), add = TRUE)

# (e) QQ‐plot
qqnorm(std_resid); qqline(std_resid)

# (f) Ljung–Box test for whiteness
Box.test(std_resid, type = "Ljung-Box", lag = 20)
```

```{r}
#| label: Parameter uncertainty

# 1. MLE with Hessian                                   (ATTENTO CHE CI METTE UN BOTTO, VALUTIAMO SE FARE SU R ONLINE CHE QUI IL TOSTAPANE SI FONDE - STEFANO)
fit_hess <- dlmMLE(ts_gistemp, parm = init, build = buildLL2S, hessian = TRUE)

# 2. Extract log‐parameters & invert Hessian
log_par <- fit_hess$par
vcov_log <- solve(fit_hess$hessian)
se_log   <- sqrt(diag(vcov_log))

# 3. Build a summary table
param_tab <- data.frame(
  param     = c("V", "W_mu", "W_beta", "W_seas"),
  log_est   = log_par,
  log_se    = se_log
)
param_tab$est     <- exp(param_tab$log_est)
param_tab$lower95 <- exp(param_tab$log_est - 1.96 * param_tab$log_se)
param_tab$upper95 <- exp(param_tab$log_est + 1.96 * param_tab$log_se)

print(param_tab)
```

```{r}
#| label: Model fit: log‐likelihood, AIC, BIC

# log‐likelihood at the MLE (dlmMLE returns val = –logLik)
logLik <- -fit_hess$value
p      <- length(log_par)
n      <- length(ts_gistemp)

AIC <- -2*logLik + 2*p
BIC <- -2*logLik + log(n)*p

cat("logLik =", logLik, "\nAIC =", AIC, "\nBIC =", BIC, "\n")
```

```{r}
#| label: Model fit comparison with pure LLT

buildLL2 <- function(par) {
  V      <- exp(par[1])
  W_mu   <- exp(par[2])
  W_beta <- exp(par[3])
  # second-order polynomial → level + slope
  dlmModPoly(order = 2,
             dV = V,
             dW = c(W_mu, W_beta))
}

# 2. Initial values (only 3 parameters now)
initLL2 <- log(c(
  var(ts_gistemp),         # V
  var(ts_gistemp)/100,     # W_mu
  var(ts_gistemp)/1000     # W_beta
))

# 3. Fit it
fitLL2  <- dlmMLE(ts_gistemp, parm = init, build = buildLL2)
modLL2  <- buildLL2(fitLL2$par)

logLik2 <- -fitLL2$value
p2      <- length(fitLL2$par)
AIC2    <- -2*logLik2 + 2*p2
BIC2    <- -2*logLik2 + log(length(ts_gistemp))*p2

cat("logLik =", logLik2, "\nAIC =", AIC2, "\nBIC =", BIC2, "\n")
cat("ΔAIC =",  AIC - AIC2,  "   ΔBIC =", BIC - BIC2)
```

```{r}
#| label: Forecast‐performance on a hold‐out | Traditional method

# (a) Define training sample ending December 2019
n_train   <- length(ts_gistemp) - 60
t_end     <- time(ts_gistemp)[n_train]
train_ts  <- window(ts_gistemp, end = t_end)

# (b) Re‐fit on training data
fit_train <- dlmMLE(train_ts, parm = init, build = buildLL2S)
mod_train <- buildLL2S(fit_train$par)
filt_train<- dlmFilter(train_ts, mod_train)

# (c) Forecast next 120 months
fc        <- dlmForecast(filt_train, nAhead = 60)
y_true    <- as.numeric(ts_gistemp[(n_train+1):length(ts_gistemp)])
y_pred    <- drop(fc$f)          # forecast means
y_lo      <- fc$a + qnorm(0.025)*sqrt(unlist(lapply(fc$Q, function(Q) Q[1,1])))
y_hi      <- fc$a + qnorm(0.975)*sqrt(unlist(lapply(fc$Q, function(Q) Q[1,1])))

# (d) Compute accuracy metrics
errors <- y_true - y_pred
RMSE   <- sqrt(mean(errors^2))
MAE    <- mean(abs(errors))
coverage <- mean((y_true >= y_lo) & (y_true <= y_hi))

cat("RMSE =", round(RMSE,3),
    " MAE =", round(MAE,3),
    " 95% CI coverage =", round(coverage*100,1), "%\n")
```

# Task 3 \| GHCN: Weather prediction

The dataset includes data along space (several stations) and time. Typical aspects of interest are spatial interpolations (at a fixed time) or, in our case, DLM models for spatio-temporal data, considering temperature over time for multiple stations (say 2, for simplicity). You are welcome to explore this direction. However, to keep your workload lighter, here is our suggestion for your possible analysis, focusing only on one station, namely `San Francisco Downtown`.

## 3.1 Data and question

Extract the seasonality adjusted minimum and maximum daily temperatures from the `San Francisco Downtown` station. We want to obtain short term predictions for the minimum and maximum temperature and to investigate a potential common latent process that describes the weather in San Francisco.

N.B.: You need to divide columns 7, 8, and 9 to obtain temperatures (`TMIN`, `TMAX`, `TAVG`) in Celsius degrees.

## 3.2 Short term temperature prediction

Explore the use of Dynamic Linear Models to obtain short term temperature predictions. Evaluate the models based on their interpretability and the quality of their predictions.

Namely, consider the bivariate time series of minimum and maximum temperature,

$$
Y_t = 
\begin{bmatrix}T_{\min,t} \\[4pt]T_{\max,t}\end{bmatrix}
$$

and explore the following models.

### 3.2.(a) Independent random walk plus noise models

$$
\begin{aligned}\mathbf{Y}_t &= \boldsymbol{\theta}_t + \mathbf{v}_t,   &\mathbf{v}_t &\sim \mathcal{N}(\mathbf{0}, \mathbf{V}),\\\boldsymbol{\theta}_t &= \boldsymbol{\theta}_{t-1} + \mathbf{w}_t,   &\mathbf{w}_t &\sim \mathcal{N}(\mathbf{0}, \mathbf{W}),\end{aligned}
$$

where $(\boldsymbol{\theta}_t = [\theta{1,t},,\theta{2,t}]^\top)$ is the latent state at time $(t)$, and $(\mathbf{W})$ and $(\mathbf{V})$ are diagonal:

$$
\mathbf{W} =\begin{bmatrix}
\sigma^2_{w,1} & 0\\
0 & \sigma^2_{w,2}
\end{bmatrix}, \qquad \mathbf{V} =\begin{bmatrix}
\sigma^2_{v,1} & 0\\
0 & \sigma^2_{v,2}
\end{bmatrix}.
$$ \> Stefano

```{r}
#| label: 1. Builder: log‐parameters = log(c(V1, V2, W1, W2))

buildIndRWNoise <- function(par) {
  V1 <- exp(par[1])
  V2 <- exp(par[2])
  W1 <- exp(par[3])
  W2 <- exp(par[4])
  
  # independent two‐dimensional random‐walk + noise
  mod_rw <- dlm(
    FF = diag(2),
    V  = diag(c(V1, V2)),
    GG = diag(2),
    W  = diag(c(W1, W2)),
    m0 = rep(0, 2),
    C0 = 1e7 * diag(2)
  )
  mod_rw
}
```

```{r}
#| label: 2. Obtaining the MLE estimates

# initial guesses: obs‐var = var(y), rw‐var = var(diff(y))
y       <- cbind(ts_tmin, ts_tmax)
init_par <- log(c(
  var(y[,1], na.rm=TRUE), 
  var(y[,2], na.rm=TRUE), 
  var(diff(y[,1]),  na.rm=TRUE), 
  var(diff(y[,2]),  na.rm=TRUE)
))

fitInd <- dlmMLE(
  y     = y,
  parm  = init_par,
  build = buildIndRWNoise,
  lower = rep(-20, 4),  # keep logs in a reasonable range
  upper = rep( 20, 4)
)

modInd <- buildIndRWNoise(fitInd$par)
```

```{r}
#| label: 3. Filtering and smoothing

filtInd  <- dlmFilter(y, modInd)
smthInd  <- dlmSmooth(filtInd)

# extract smoothed states (drop the t=0 initialization)
s_all    <- smthInd$s[-1, ]        # T×2 matrix: columns = (θ₁,t, θ₂,t)
tmin_smth <- s_all[, 1]            # smoothed θ₁,t for Tmin
tmax_smth <- s_all[, 2]            # smoothed θ₂,t for Tmax

# retrieve estimated noise variances
V_est <- modInd$V
W_est <- modInd$W

cat("Estimated observation variances (V):\n")
print(V_est)
cat("Estimated state‐innovation variances (W):\n")
print(W_est)
```

```{r}
#| fig-width: 12
#| fig-height: 12

# 1. Compute smoothing covariances → standard deviations
Ct_list    <- dlmSvd2var(smthInd$U.S, smthInd$D.S)
var_tmin   <- sapply(Ct_list, function(mat) mat[1,1])
sd_tmin    <- sqrt(var_tmin)[-1]
var_tmax   <- sapply(Ct_list, function(mat) mat[2,2])
sd_tmax    <- sqrt(var_tmax)[-1]

# 2. Build 95% credible intervals via FFBS
set.seed(123)
n.sims     <- 50                                                                # Questo lo aumentiamo più tardi, senò ci mette troppo
sims_array <- replicate(
  n.sims,
  {
    fd <- dlmBSample(filtInd)
    rbind(
      tmin = as.numeric(fd[-1, 1]),
      tmax = as.numeric(fd[-1, 2])
    )
  },
  simplify = "array"
)

lower95_tmin <- apply(sims_array["tmin", , ], 1, quantile, probs = 0.025)
upper95_tmin <- apply(sims_array["tmin", , ], 1, quantile, probs = 0.975)
lower95_tmax <- apply(sims_array["tmax", , ], 1, quantile, probs = 0.025)
upper95_tmax <- apply(sims_array["tmax", , ], 1, quantile, probs = 0.975)

# 3. Time index for daily series
time_idx <- seq(
  from       = start(ts_tmin)[1] + (start(ts_tmin)[2] - 1) / 365,
  by         = 1/365,
  length.out = length(ts_tmin)
)

# 4. Prepare data frame for plotting
df_plot <- data.frame(
  Time     = rep(time_idx, 2),
  Observed = c(as.numeric(ts_tmin),    as.numeric(ts_tmax)),
  Smoothed = c(smthInd$s[-1,1],        smthInd$s[-1,2]),
  Lower95  = c(lower95_tmin,           lower95_tmax),
  Upper95  = c(upper95_tmin,           upper95_tmax),
  Series   = rep(c("Tmin", "Tmax"), each = length(time_idx))
)

# 5. Plot with facets
library(ggplot2)

ggplot(df_plot, aes(x = Time)) +
  geom_line(aes(y = Observed), color = "orange",    linewidth = 0.7) +
  geom_line(aes(y = Smoothed), color = "steelblue", linewidth = 0.5) +
  geom_ribbon(aes(ymin = Lower95, ymax = Upper95),
              fill = "steelblue", alpha = 0.2) +
  facet_wrap(~Series, ncol = 1, scales = "free_y") +
  labs(
    title    = "Smoothed Random‐Walk Levels for Tmin & Tmax",
    subtitle = "95% credible intervals around the smoothed θ_t",
    x        = "Year",
    y        = "Temperature (°C)"
  ) +
  theme_minimal() +
  theme(
    plot.title    = element_text(size = 16, face = "bold"),
    plot.subtitle = element_text(size = 12, face = "italic")
  )
```

#### 3.2.(a).(i) Evaluating the model

```{r}
#| label: Residual diagnostics
#| fig-width: 12
#| fig-height: 12

# (a) raw and standardized one‐step–ahead errors
raw_resid <- residuals(filtInd, type = "raw")
std_resid <- residuals(filtInd, type = "standardized", sd = FALSE)

# (b) Plot standardized residuals over time
par(mfrow = c(2,1))
ts.plot(std_resid[,1],
        ylab = "Std. Residuals (Tmin)",
        main = "Standardized One-step-Ahead Errors: Tmin")
abline(h = c(-2,2), col = "firebrick", lty = 2)
ts.plot(std_resid[,2],
        ylab = "Std. Residuals (Tmax)",
        main = "Standardized One-step-Ahead Errors: Tmax")
abline(h = c(-2,2), col = "firebrick", lty = 2)

# (c) ACF to check for remaining autocorrelation
acf(std_resid[,1], main = "ACF of Std. Residuals: Tmin")
acf(std_resid[,2], main = "ACF of Std. Residuals: Tmax")

# (d) Histogram + normal curve
par(mfrow = c(2,1))
hist(std_resid[,1], freq = FALSE,
     main = "Histogram of Std. Residuals: Tmin",
     xlab = "Std. Residual")
curve(dnorm(x), add = TRUE)
hist(std_resid[,2], freq = FALSE,
     main = "Histogram of Std. Residuals: Tmax",
     xlab = "Std. Residual")
curve(dnorm(x), add = TRUE)

# (e) QQ‐plot
par(mfrow = c(2,1))
qqnorm(std_resid[,1], main = "QQ‐plot: Std. Residuals (Tmin)"); qqline(std_resid[,1])
qqnorm(std_resid[,2], main = "QQ‐plot: Std. Residuals (Tmax)"); qqline(std_resid[,2])

# (f) Ljung–Box test for whiteness
lb_tmin <- Box.test(std_resid[,1], type = "Ljung-Box", lag = 20)
lb_tmax <- Box.test(std_resid[,2], type = "Ljung-Box", lag = 20)
print(lb_tmin)
print(lb_tmax)
```

```{r}
#| label: Parameter uncertainty

# 1. MLE with Hessian (may be slow)
fit_hess <- dlmMLE(
  y     = y,
  parm  = init_par,
  build = buildIndRWNoise,
  hessian = TRUE
)

# 2. Extract log‐parameters & invert Hessian
log_par  <- fit_hess$par
vcov_log <- solve(fit_hess$hessian)
se_log   <- sqrt(diag(vcov_log))

# 3. Summary table
param_tab <- data.frame(
  param     = c("V1", "V2", "W1", "W2"),
  log_est   = log_par,
  log_se    = se_log
)
param_tab$est     <- exp(param_tab$log_est)
param_tab$lower95 <- exp(param_tab$log_est - 1.96 * param_tab$log_se)
param_tab$upper95 <- exp(param_tab$log_est + 1.96 * param_tab$log_se)
print(param_tab)
```

```{r}
#| label: Model fit: log‐likelihood, AIC, BIC

logLik <- -fit_hess$value
p      <- length(log_par)
n      <- nrow(y)

AIC <- -2*logLik + 2*p
BIC <- -2*logLik + log(n)*p

cat("logLik =", logLik, 
    "\nAIC    =", AIC, 
    "\nBIC    =", BIC, "\n")
```

```{r}
#| label: Forecast performance (hold‐out)

# (a) Define training sample ending 1 year (~365 days) before end
n_train <- nrow(y) - 365
y_train <- y[1:n_train, , drop = FALSE]

# (b) Re‐fit on training data
fit_train  <- dlmMLE(y_train, parm = init_par, build = buildIndRWNoise)
mod_train  <- buildIndRWNoise(fit_train$par)
filt_train <- dlmFilter(y_train, mod_train)

# (c) Forecast next 365 days
fc      <- dlmForecast(filt_train, nAhead = 365)
y_true  <- y[(n_train+1):nrow(y), ]
y_pred  <- fc$f
Q_list  <- fc$Q

# prediction intervals
y_lo <- matrix(NA, nrow = 365, ncol = 2)
y_hi <- matrix(NA, nrow = 365, ncol = 2)
for(i in 1:365) {
  se_pred     <- sqrt(diag(Q_list[[i]]))
  y_lo[i, ]   <- y_pred[i, ] + qnorm(0.025) * se_pred
  y_hi[i, ]   <- y_pred[i, ] + qnorm(0.975) * se_pred
}

# (d) Compute accuracy metrics by series
errors   <- y_true - y_pred
rmse     <- sqrt(colMeans(errors^2))
mae      <- colMeans(abs(errors))
coverage <- colMeans((y_true >= y_lo) & (y_true <= y_hi)) * 100

cat("Series   RMSE    MAE    95% CI coverage\n")
cat(sprintf("Tmin   %.3f  %.3f   %.1f%%\n", rmse[1], mae[1], coverage[1]))
cat(sprintf("Tmax   %.3f  %.3f   %.1f%%\n", rmse[2], mae[2], coverage[2]))
```

### 3.2.(b) "Seemingly unrelated" random walk plus noise models (V diagonal and W full)

$$
\begin{aligned}
\mathbf{Y}_t &= \boldsymbol{\theta}_t + \mathbf{v}_t, 
  &\mathbf{v}_t &\sim \mathcal{N}(\mathbf{0}, \mathbf{V}),\\
\boldsymbol{\theta}_t &= \boldsymbol{\theta}_{t-1} + \mathbf{w}_t, 
  &\mathbf{w}_t &\sim \mathcal{N}(\mathbf{0}, \mathbf{W}),
\end{aligned}
$$ where

$$
\mathbf{W} = 
\begin{bmatrix}
\sigma^2_{w,11} & \sigma^2_{w,12}\\[3pt]
\sigma^2_{w,21} & \sigma^2_{w,22}
\end{bmatrix},
\qquad
\mathbf{V} = 
\begin{bmatrix}
\sigma^2_{v,11} & 0\\
0 & \sigma^2_{v,22}
\end{bmatrix}.
$$

```{r}
#| label: 1. Model‐building function

build_SUTSE <- function(par) {
  V <- diag(exp(par[1:2]))
  W <- matrix(c(exp(par[3]), par[4],
                par[4],      exp(par[5])),
              2, 2, byrow = TRUE)
  dlm(
    FF = diag(2), V = V,
    GG = diag(2), W = W,
    m0 = rep(0,2), C0 = diag(1e7,2)
  )
}
temp <- cbind(Tmin = ts_tmin, Tmax = ts_tmax)

```

```{r}
#| label: 2. fit by MLE

init.par <- rep(0,5)
mle   <- dlmMLE(y = temp, parm = init.par, build = build_SUTSE)
mod   <- build_SUTSE(mle$par)
if (mle$convergence != 0)
  warning("MLE did not converge: code ", mle$convergence)
```

```{r}
#| label: 3. Filtering and smoothing

filt <- dlmFilter(temp, mod)
smth <- dlmSmooth(filt)

raw    <- residuals(filt, type = "raw", sd = TRUE)
res_raw <- raw$res     # n×2 ts of one-step-ahead errors y_t - ŷ_{t|t-1}
pred_se <- raw$sd      # n×2 ts of predictive standard deviations

# reconstruct the in‐sample forecasts 
y_pred <- temp - res_raw

# standardize residuals 
res_std <- res_raw / pred_se

```

#### 3.2.(b).(i) Evaluating the model

```{r}
#| label: 1. Diagnostic plots

par(mfrow = c(2,2), mar = c(4,4,2,1))
ts.plot(res_std[,1], main="Std. resid: Tmin"); abline(h=0, col="gray")
ts.plot(res_std[,2], main="Std. resid: Tmax"); abline(h=0, col="gray")
acf(res_std[,1], main="ACF Std resid: Tmin")
acf(res_std[,2], main="ACF Std resid: Tmax")

par(mfrow = c(1,2))
qqnorm(res_std[,1], main="QQ: Std resid Tmin"); qqline(res_std[,1])
qqnorm(res_std[,2], main="QQ: Std resid Tmax"); qqline(res_std[,2])
checkresiduals(res_std[,1])
checkresiduals(res_std[,2])
```

```{r}
#| label: 2. Forecast accuracy metrics

ok  <- complete.cases(res_raw)   # drop initial NA
e   <- res_raw[ok,]              # raw errors
se  <- pred_se[ok,]              # predictive SD

rmse <- sqrt(colMeans(e^2))
mae  <- colMeans(abs(e))

lower <- y_pred - 1.96 * pred_se
upper <- y_pred + 1.96 * pred_se
cov  <- sapply(1:2, function(i)
  mean(temp[ok,i] >= lower[ok,i] & temp[ok,i] <= upper[ok,i]) * 100
)
```

```{r}
#| label: 3. Print summary

cat("Series   RMSE    MAE   95% PI Coverage\n")
cat(sprintf("Tmin   %6.3f %6.3f    %5.1f%%\n",
            rmse[1], mae[1], cov[1]))
cat(sprintf("Tmax   %6.3f %6.3f    %5.1f%%\n",
            rmse[2], mae[2], cov[2]))

colnames(y_pred) <- colnames(temp)
```

```{r}
#| label: 4. Build dataframe for plotting
df <- data.frame(
  time      = as.numeric(time(temp)),
  Tmin      = as.numeric(temp[, "Tmin"]),
  Tmax      = as.numeric(temp[, "Tmax"]),
  Tmin_pred = as.numeric(y_pred[, "Tmin"]),
  Tmax_pred = as.numeric(y_pred[, "Tmax"])
)

# melt into long form
df_melt <- melt(df, id.vars = "time", 
                variable.name = "Series_Source", 
                value.name    = "Value")

# split into Series (Tmin/Tmax) and Source (Original/Estimated)
df_melt$Series <- ifelse(grepl("^Tmin", df_melt$Series_Source), "Tmin", "Tmax")
df_melt$Source <- ifelse(grepl("_pred$", df_melt$Series_Source), 
                         "Estimated", "Original")
```

```{r}
ggplot(df_melt, aes(x = time, y = Value, color = Source)) +
  geom_line(size = 0.8) +
  facet_wrap(~ Series, scales = "free_y", ncol = 1) +
  scale_color_manual(values = c("Original" = "black", 
                                "Estimated" = "firebrick")) +
  labs(
    x     = "Time (year)",
    y     = "Temperature",
    color = "",
    title = "Original vs One‐Step‐Ahead DLM Estimates"
  ) +
  theme_bw(base_size = 14) +
  theme(
    legend.position  = "top",
    strip.text       = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey80")
  )
```

```{r}
#| label: Making a subsample of the previous dataset to make it easier to plot and interpret
df <- data.frame(
  time      = as.numeric(time(temp)),
  Tmin      = as.numeric(temp[,"Tmin"]),
  Tmax      = as.numeric(temp[,"Tmax"]),
  Tmin_pred = as.numeric(y_pred[,"Tmin"]),
  Tmax_pred = as.numeric(y_pred[,"Tmax"])
)
df_sub <- subset(df, time >= 1990 & time <= 1991)
df_melt <- melt(df_sub, id.vars = "time",
                variable.name = "Series_Source",
                value.name    = "Value")
df_melt$Series <- ifelse(grepl("^Tmin", df_melt$Series_Source), "Tmin", "Tmax")
df_melt$Source <- ifelse(grepl("_pred$", df_melt$Series_Source),
                         "Estimated", "Original")
```

```{r}
#| label: Refined plot
ggplot(df_melt, aes(x = time, y = Value, 
                    color = Source, linetype = Source)) +
  geom_line(aes(size = Source)) +
  facet_wrap(~ Series, scales = "free_y", ncol = 1) +
  scale_color_manual(values = c("Original" = "black", "Estimated" = "firebrick")) +
  scale_linetype_manual(values = c("Original" = "solid", "Estimated" = "solid")) +
  scale_size_manual(values = c("Original" = 1, "Estimated" = 1)) +
  labs(
    x     = "Time (year)",
    y     = "Temperature",
    color = "",
    linetype = "",
    size = "",
    title = "Original vs. One‐Step‐Ahead DLM Estimates 1990-1991)"
  ) +
  theme_bw(base_size = 14) +
  theme(
    legend.position  = "top",
    strip.text       = element_text(face = "bold", size = 14),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey80")
  )
```

```{r}
#add credible intervals 95%
#--- load packages ----------------------------------------------------------

#--- 1) Create df for observed & predicted, subset to 1990–1991 -------------
df <- data.frame(
    time       = as.numeric(time(temp)),         # numeric time index for `temp`
    Tmin       = as.numeric(temp[,"Tmin"]),       # observed Tmin
    Tmax       = as.numeric(temp[,"Tmax"]),       # observed Tmax
    Tmin_pred  = as.numeric(y_pred[,"temp.Tmin"]),     # predicted Tmin
    Tmax_pred  = as.numeric(y_pred[,"temp.Tmax"])      # predicted Tmax
)

df_sub <- subset(df, time >= 1990 & time <= 1991)

df_melt <- melt(
    df_sub,
    id.vars        = "time",
    variable.name  = "Series_Source",
    value.name     = "Value"
) %>%
    mutate(
        Series = ifelse(grepl("^Tmin",   Series_Source), "Tmin", "Tmax"),
        Source = ifelse(grepl("_pred$",  Series_Source), "Estimated", "Original")
    )

#--- 2) Create df for lower/upper bounds, subset to 1990–1991 --------------
#    We assume `lower` and `upper` are ts objects with the same time index as `temp`
#    and with columns "Tmin" and "Tmax".  
ci_df <- data.frame(
    time        = as.numeric(time(lower)),
    Tmin_lower  = as.numeric(lower[,"y_pred.temp.Tmin"]),
    Tmin_upper  = as.numeric(upper[,"y_pred.temp.Tmin"]),
    Tmax_lower  = as.numeric(lower[,"y_pred.temp.Tmax"]),
    Tmax_upper  = as.numeric(upper[,"y_pred.temp.Tmax"])
)

ci_sub <- subset(ci_df, time >= 1990 & time <= 1991)

# Pivot into long form with columns (time, Series, lower, upper)
ci_melt <- bind_rows(
    ci_sub %>% transmute(time, Series = "Tmin",
                         lower = Tmin_lower, upper = Tmin_upper),
    ci_sub %>% transmute(time, Series = "Tmax",
                         lower = Tmax_lower, upper = Tmax_upper)
)

#--- 3) Plot everything -----------------------------------------------------
ggplot() +
    # (a) 95% prediction‐interval ribbon in grey behind the lines
    geom_ribbon(
        data = ci_melt,
        aes(x = time, ymin = lower, ymax = upper),
        fill  = "grey40",
        alpha = 0.3
    ) +
    # (b) observed vs. predicted lines on top
    geom_line(
        data = df_melt,
        aes(x = time, y = Value,
            color = Source, linetype = Source),
        size = 1
    ) +
    facet_wrap(~ Series, scales = "free_y", ncol = 1) +
    scale_color_manual(
        values = c("Original" = "black", "Estimated" = "firebrick")
    ) +
    scale_linetype_manual(
        values = c("Original" = "solid", "Estimated" = "solid")
    ) +
    labs(
        x     = "Time (year)",
        y     = "Temperature",
        title = "Original vs One‐Step‐Ahead DLM (1990–1991) with 95% Intervals"
    ) +
    theme_bw(base_size = 14) +
    theme(
        legend.position  = "top",
        strip.text       = element_text(face = "bold", size = 14),
        panel.grid.minor = element_blank(),
        panel.grid.major = element_line(color = "grey80")
    )

```

```{r}
# 1. MLE with Hessian (may be slow)
fit_hess2 <- dlmMLE(
  y     = temp,
  parm  = init.par,
  build = build_SUTSE,
  hessian = TRUE
)

# 2. Extract log‐parameters & invert Hessian
log_par  <- fit_hess2$par
vcov_log <- solve(fit_hess2$hessian)
se_log   <- sqrt(diag(vcov_log))

# 3. Summary table
param_tab <- data.frame(
  param     = c("V1", "V2", "W1", "W1-2", "W2"),
  log_est   = log_par,
  log_se    = se_log
)
param_tab$est     <- exp(param_tab$log_est)
param_tab$lower95 <- exp(param_tab$log_est - 1.96 * param_tab$log_se)
param_tab$upper95 <- exp(param_tab$log_est + 1.96 * param_tab$log_se)
print(param_tab)
```

```{r}
logLik <- -fit_hess2$value
p      <- length(log_par)
n      <- nrow(temp)

AIC <- -2*logLik + 2*p
BIC <- -2*logLik + log(n)*p

cat("logLik =", logLik, 
    "\nAIC    =", AIC, 
    "\nBIC    =", BIC, "\n")
```

```{r}
#forecast out of sample
#| label: Forecast performance (hold‐out)

# (a) Define training sample ending 1 year (~365 days) before end
n_train <- nrow(temp) - 365
y_train <- temp[1:n_train, , drop = FALSE]

# (b) Re‐fit on training data
fit_train  <- dlmMLE(y_train, parm = init.par, build = build_SUTSE)
mod_train  <- build_SUTSE(fit_train$par)
filt_train <- dlmFilter(y_train, mod_train)

# (c) Forecast next 365 days
fc      <- dlmForecast(filt_train, nAhead = 365)
y_true  <- temp[(n_train+1):nrow(temp), ]
y_pred  <- fc$f
Q_list  <- fc$Q

# prediction intervals
y_lo <- matrix(NA, nrow = 365, ncol = 2)
y_hi <- matrix(NA, nrow = 365, ncol = 2)
for(i in 1:365) {
  se_pred     <- sqrt(diag(Q_list[[i]]))
  y_lo[i, ]   <- y_pred[i, ] + qnorm(0.025) * se_pred
  y_hi[i, ]   <- y_pred[i, ] + qnorm(0.975) * se_pred
}

# (d) Compute accuracy metrics by series
errors   <- y_true - y_pred
rmse     <- sqrt(colMeans(errors^2))
mae      <- colMeans(abs(errors))
coverage <- colMeans((y_true >= y_lo) & (y_true <= y_hi)) * 100

cat("Series   RMSE    MAE    95% CI coverage\n")
cat(sprintf("Tmin   %.3f  %.3f   %.1f%%\n", rmse[1], mae[1], coverage[1]))
cat(sprintf("Tmax   %.3f  %.3f   %.1f%%\n", rmse[2], mae[2], coverage[2]))
```

### 3.2.(c) Random walks plus noise driven by a "latent factor" (a common state process)

$$
\mathbf{Y}_t = \mathbf{F}\,\boldsymbol{\theta}_t + \mathbf{v}_t,\quad
\mathbf{v}_t \sim \mathcal{N}(\mathbf{0},\,\mathbf{V}),
$$ $$
\boldsymbol{\theta}_t = \boldsymbol{\theta}_{t-1} + 
\begin{pmatrix}
0 \\[4pt]
w_t
\end{pmatrix},
\quad
w_t \sim \mathcal{N}(0,\sigma_w^2),
$$

where

$$
\boldsymbol{\theta}_t =
\begin{bmatrix}
1 \\[4pt]
\xi_t
\end{bmatrix},\quad
\mathbf{F} =
\begin{bmatrix}
\alpha_1 & \beta \\[6pt]
\alpha_2 & \frac{1}{\beta}
\end{bmatrix},\quad
\mathbf{V} =
\begin{bmatrix}
\sigma^2_{v,11} & 0\\[4pt]
0 & \sigma^2_{v,22}
\end{bmatrix}.
$$ The parameters $\alpha_1$, $\alpha_2$ and $\beta$ should be determined via MLE.

```{r}
#| label: 1. Builder: log‐parameters = c(α1, α2, logβ, logσv1, logσv2, logσw)
buildCF <- function(par) {
  # raw parameters
  alpha1  <- par[1]
  alpha2  <- par[2]
  # positivity constraints
  beta     <- exp(par[3])
  sigma_v1 <- exp(par[4])
  sigma_v2 <- exp(par[5])
  sigma_w  <- exp(par[6])
  
  # Observation matrix F
  FF <- matrix(c(
    alpha1,     beta,
    alpha2, 1 / beta
  ), nrow = 2, byrow = TRUE)
  
  # Observation noise covariance V
  V  <- diag(c(sigma_v1^2, sigma_v2^2))
  
  # State evolution G = identity, with W only on second component
  GG <- diag(2)
  W  <- diag(c(0, sigma_w^2))
  
  # Initial state mean m0 = [1, 0]′, large variance on ξ₀
  m0 <- c(1, 0)
  C0 <- diag(c(0, 1e7))
  
  dlm(
    m0 = m0, C0 = C0,
    FF = FF, V = V,
    GG = GG, W = W
  )
}
```

```{r}
#| label: 2. Obtaining the MLE estimates
# stack Tmin and Tmax
y      <- cbind(ts_tmin, ts_tmax)

# sensible initials:
# α’s ≈ 0, β ≈ 1, σv ≈ sd(y), σw ≈ sd(diff of common factor)
init_par <- c(
  alpha1 = 0,
  alpha2 = 0,
  logβ    = log(1),
  logσv1  = log(sd(y[,1], na.rm=TRUE)),
  logσv2  = log(sd(y[,2], na.rm=TRUE)),
  logσw   = log(sd(diff(y[,1] + y[,2])/2, na.rm=TRUE))
)

fitCF <- dlmMLE(
  y     = y,
  parm  = init_par,
  build = buildCF,
  lower = c(-Inf, -Inf, -10, -10, -10, -10),
  upper = c( Inf,  Inf,  10,  10,  10,  10)
)
if (fitCF$convergence != 0) stop("MLE did not converge")

modCF <- buildCF(fitCF$par)
```

```{r}
#| label: 3. Filtering and smoothing
filtCF  <- dlmFilter(y, modCF)
smthCF  <- dlmSmooth(filtCF)

# extract smoothed states (drop t=0)
s_all    <- smthCF$s[-1, ]    # T × 2 matrix
xi_smth  <- s_all[, 2]        # the latent ξₜ

# retrieve estimated parameters
FF_est <- modCF$FF
V_est  <- modCF$V
W_est  <- modCF$W

cat("Estimated F matrix:\n")
print(FF_est)
cat("\nEstimated observation noise V:\n")
print(V_est)
cat("\nEstimated state‐innovation noise W:\n")
print(W_est)

# optional: plot the latent factor
ts.plot(xi_smth, main="Smoothed latent factor ξₜ")
```

#### 3.2.(c).(i) Evaluating the model

```{r}
#| label: Residual diagnostics: CF model
#| fig-width: 16
#| fig-height: 16

# (a) raw and standardized one‐step–ahead errors
raw_resid_CF <- residuals(filtCF, type = "raw")
std_resid_CF <- residuals(filtCF, type = "standardized", sd = FALSE)

# (b) Plot standardized residuals over time
par(mfrow = c(2,1))
ts.plot(std_resid_CF[,1],
        ylab = "Std. Residuals (Tmin)",
        main = "CF Model: Standardized One-step-Ahead Errors: Tmin")
abline(h = c(-2,2), col = "firebrick", lty = 2)
ts.plot(std_resid_CF[,2],
        ylab = "Std. Residuals (Tmax)",
        main = "CF Model: Standardized One-step-Ahead Errors: Tmax")
abline(h = c(-2,2), col = "firebrick", lty = 2)

# (c) ACF to check for remaining autocorrelation
acf(std_resid_CF[,1], main = "CF Model: ACF of Std. Residuals: Tmin")
acf(std_resid_CF[,2], main = "CF Model: ACF of Std. Residuals: Tmax")

# (d) Histogram + normal curve
par(mfrow = c(2,1))
hist(std_resid_CF[,1], freq = FALSE,
     main = "CF Model: Histogram of Std. Residuals: Tmin",
     xlab = "Std. Residual")
curve(dnorm(x), add = TRUE)
hist(std_resid_CF[,2], freq = FALSE,
     main = "CF Model: Histogram of Std. Residuals: Tmax",
     xlab = "Std. Residual")
curve(dnorm(x), add = TRUE)

# (e) QQ‐plot
par(mfrow = c(2,1))
qqnorm(std_resid_CF[,1], main = "CF Model: QQ‐plot: Std. Residuals (Tmin)"); qqline(std_resid_CF[,1])
qqnorm(std_resid_CF[,2], main = "CF Model: QQ‐plot: Std. Residuals (Tmax)"); qqline(std_resid_CF[,2])

# (f) Ljung–Box test for whiteness
lb_tmin_CF <- Box.test(std_resid_CF[,1], type = "Ljung-Box", lag = 20)
lb_tmax_CF <- Box.test(std_resid_CF[,2], type = "Ljung-Box", lag = 20)
print(lb_tmin_CF)
print(lb_tmax_CF)
```

```{r}
#| label: Parameter uncertainty: CF model

# 1. MLE with Hessian (slower)
fitCF_hess <- dlmMLE(
  y       = y,
  parm    = init_par,
  build   = buildCF,
  hessian = TRUE
)

# 2. Extract estimated parameters and their SEs on the working scale
par_work   <- fitCF_hess$par
vcov_work  <- solve(fitCF_hess$hessian)
se_work    <- sqrt(diag(vcov_work))

# 3. Construct summary table
param_tab_CF <- data.frame(
  param      = c("alpha1", "alpha2", "logβ", "logσv1", "logσv2", "logσw"),
  estimate   = par_work,
  se         = se_work,
  lower95    = par_work - 1.96 * se_work,
  upper95    = par_work + 1.96 * se_work
)
print(param_tab_CF)
```

```{r}
#| label: Model fit: CF model log‐likelihood, AIC, BIC

logLik_CF <- -fitCF_hess$value
p_CF      <- length(par_work)
n_CF      <- nrow(y)

AIC_CF <- -2 * logLik_CF + 2 * p_CF
BIC_CF <- -2 * logLik_CF + log(n_CF) * p_CF

cat("CF model\n",
    "logLik =", round(logLik_CF,2), 
    "\nAIC    =", round(AIC_CF,2), 
    "\nBIC    =", round(BIC_CF,2), "\n")
```

```{r}
#| label: Forecast performance (hold‐out): CF model

# (a) Training sample ends 365 days before end
n_train_CF <- nrow(y) - 365
y_train_CF <- y[1:n_train_CF, , drop = FALSE]

# (b) Re‐fit CF model on training data
fit_train_CF  <- dlmMLE(y_train_CF, parm = init_par, build = buildCF)
mod_train_CF  <- buildCF(fit_train_CF$par)
filt_train_CF <- dlmFilter(y_train_CF, mod_train_CF)

# (c) Forecast next 365 days
fc_CF     <- dlmForecast(filt_train_CF, nAhead = 365)
y_true_CF <- y[(n_train_CF+1):nrow(y), ]
y_pred_CF <- fc_CF$f
Q_list_CF <- fc_CF$Q

# prediction intervals
y_lo_CF <- y_hi_CF <- matrix(NA, nrow = 365, ncol = 2)
for(i in 1:365) {
  se_pred     <- sqrt(diag(Q_list_CF[[i]]))
  y_lo_CF[i,] <- y_pred_CF[i,] + qnorm(0.025) * se_pred
  y_hi_CF[i,] <- y_pred_CF[i,] + qnorm(0.975) * se_pred
}

# (d) Compute accuracy metrics by series
errors_CF  <- y_true_CF - y_pred_CF
rmse_CF    <- sqrt(colMeans(errors_CF^2))
mae_CF     <- colMeans(abs(errors_CF))
coverage_CF<- colMeans((y_true_CF >= y_lo_CF) & (y_true_CF <= y_hi_CF)) * 100

cat("CF model forecast performance:\n")
cat(sprintf("Series   RMSE    MAE    95%% CI coverage\n"))
cat(sprintf("Tmin   %.3f  %.3f   %.1f%%\n", rmse_CF[1], mae_CF[1], coverage_CF[1]))
cat(sprintf("Tmax   %.3f  %.3f   %.1f%%\n", rmse_CF[2], mae_CF[2], coverage_CF[2]))
```

```{r}
#| label: Further checks on forecast performance (hold‐out): CF model
# compute average half‐width of your 95% intervals
half_width <- rowMeans((y_hi_CF - y_lo_CF)/2)
summary(half_width)

```
